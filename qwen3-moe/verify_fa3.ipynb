{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b493d22",
   "metadata": {},
   "source": [
    "# Install FA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pytorch 2.7.1\n",
    "# Use CUDA 12.8\n",
    "# Python > 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure rename or else your PIP is screaming\n",
    "# eg remove `-2.7.1-12.8` PyTorch CUDA version\n",
    "# check all compatible version and architecture in https://huggingface.co/datasets/malaysia-ai/Flash-Attention3-wheel/tree/main\n",
    "# below just an example for PyTorch 2.7.1 CUDA 12.8 x64\n",
    "\n",
    "!wget https://huggingface.co/datasets/mesolitica/Flash-Attention3-whl/resolve/main/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64-2.7.1-12.8.whl -O flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl\n",
    "!pip3 install flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb8058",
   "metadata": {},
   "source": [
    "# Check FA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn\n",
    "print(flash_attn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46a236",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b03ff3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7235b504",
   "metadata": {},
   "source": [
    "# Test the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d22de3",
   "metadata": {},
   "source": [
    "import inspect, types\n",
    "import torch.nn as nn\n",
    "\n",
    "def describe_attention(model):\n",
    "    for name, mod in model.named_modules():\n",
    "        if \"attn\" in name.lower() or \"self_attn\" in name.lower():\n",
    "            fwd = getattr(mod, \"forward\", None)\n",
    "            if isinstance(fwd, types.MethodType):\n",
    "                fn = fwd.__func__\n",
    "            else:\n",
    "                fn = fwd\n",
    "            origin = None\n",
    "            try:\n",
    "                origin = inspect.getsourcefile(fn)\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(f\"[{name}] class={mod.__class__.__name__}\")\n",
    "            print(\"  forward id:\", id(fn), \" file:\", origin)\n",
    "\n",
    "describe_attention(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4102c",
   "metadata": {},
   "source": [
    "# Flash Attention 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d8149",
   "metadata": {},
   "source": [
    "import flash_attn_interface\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import flash_attn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_list_sum_n(n, length=5, min_val=5):\n",
    "\n",
    "    numbers = [min_val] * length\n",
    "    remaining = n - min_val * length\n",
    "\n",
    "    for _ in range(remaining):\n",
    "        numbers[random.randint(0, length - 1)] += 1\n",
    "\n",
    "    random.shuffle(numbers)\n",
    "    return numbers\n",
    "\n",
    "def block_diagonal_concat_inverted(*masks, dtype=torch.bfloat16):\n",
    "    total_size = sum(mask.size(0) for mask in masks)\n",
    "    combined_mask = torch.zeros(total_size, total_size, dtype=dtype)\n",
    "\n",
    "    current_pos = 0\n",
    "\n",
    "    for mask in masks:\n",
    "        size = mask.size(0)\n",
    "        combined_mask[current_pos:current_pos + size, current_pos:current_pos + size] = mask\n",
    "        current_pos += size\n",
    "\n",
    "    min_value = torch.finfo(dtype).min if dtype.is_floating_point else torch.iinfo(dtype).min\n",
    "    inverted_mask = torch.where(combined_mask == 1, torch.tensor(0, dtype=dtype), min_value)\n",
    "    return inverted_mask.unsqueeze(0)\n",
    "\n",
    "sequence_length = 4096\n",
    "query_lens = np.array(generate_list_sum_n(sequence_length, length=20, min_val=10), dtype=np.int64)\n",
    "min_dtype = torch.finfo(torch.bfloat16).min\n",
    "masking = query_lens\n",
    "masks = []\n",
    "for m in masking:\n",
    "    masks.append(torch.tril(torch.ones(m, m)))\n",
    "attention_mask = block_diagonal_concat_inverted(*masks).cuda()\n",
    "\n",
    "q = torch.randn(1, sequence_length, 128, 128, dtype = torch.bfloat16).cuda()\n",
    "k = torch.randn(1, sequence_length, 128, 128, dtype = torch.bfloat16).cuda()\n",
    "v = torch.randn(1, sequence_length, 128, 128, dtype = torch.bfloat16).cuda()\n",
    "\n",
    "out_sdpa = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query = q.transpose(1, 2),\n",
    "    key = k.transpose(1, 2),\n",
    "    value = v.transpose(1, 2),\n",
    "    attn_mask = attention_mask[None],\n",
    ")\n",
    "\n",
    "cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "max_cumsum = int(np.max(cumsum))\n",
    "cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32).cuda()\n",
    "max_seqlen_q = np.max(query_lens)\n",
    "\n",
    "out_flash2 = flash_attn.flash_attn_varlen_func(\n",
    "    q = q[0],\n",
    "    k = k[0],\n",
    "    v = v[0],\n",
    "    cu_seqlens_q = cu_seq_lens_q,\n",
    "    cu_seqlens_k = cu_seq_lens_q,\n",
    "    max_seqlen_q = max_seqlen_q,\n",
    "    max_seqlen_k = max_seqlen_q,\n",
    "    causal = True\n",
    ")\n",
    "\n",
    "out_flash3 = flash_attn_interface.flash_attn_varlen_func(\n",
    "    q = q[0],\n",
    "    k = k[0],\n",
    "    v = v[0],\n",
    "    cu_seqlens_q = cu_seq_lens_q,\n",
    "    cu_seqlens_k = cu_seq_lens_q,\n",
    "    max_seqlen_q = max_seqlen_q,\n",
    "    max_seqlen_k = max_seqlen_q,\n",
    "    causal = True,\n",
    ")\n",
    "\n",
    "assert torch.allclose(out_flash3, out_sdpa[0].transpose(0, 1), atol=0.125, rtol=0)\n",
    "assert torch.allclose(out_flash3, out_flash2, atol=0.125, rtol=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
