{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524c63ff-04b0-4022-89e0-b5c18a2ff022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from glob import glob\n",
    "from transformers import GptOssForCausalLM, AutoModelForCausalLM, AutoTokenizer, Mxfp4Config, TextStreamer\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9fe1dc-6d62-412e-ade4-4586dc709066",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c35097d-abb3-47cc-930d-83444cb54296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gfs-01be5b33-gpt-oss-120b-BF16/315-model_state_dict.pt',\n",
       " 'gfs-01be5b33-gpt-oss-120b-BF16/631-model_state_dict.pt',\n",
       " 'gfs-01be5b33-gpt-oss-120b-BF16/947-model_state_dict.pt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = sorted(glob('gfs-01be5b33-gpt-oss-120b-BF16/*'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26292143-c470-415c-93ba-b50acc18f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20412a7a7dd04e5b95145ab71344a6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a08fba0456404ea1df62afcd1a44f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5444998344774661a27c2ac93ca889b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_kwargs = dict(\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    use_cache=True,\n",
    "    device_map='auto',\n",
    "    dtype='auto',\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gfs/01be5b33/gpt-oss-120b-BF16\", **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce9df0e-6d13-433c-8769-254b72d44a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58293ebd-2872-4a94-99c6-e84100309a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = files[0]\n",
    "mapping = torch.load(f, map_location='cpu')\n",
    "keys = mapping.keys()\n",
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a893e5-b9e3-44e2-ad04-7dc58fd1327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:01<00:00, 23.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(model.config.num_hidden_layers)):\n",
    "    A = f'model.layers.{i}.mlp.experts.gate_lora.A'\n",
    "    if A in mapping:\n",
    "        B = f'model.layers.{i}.mlp.experts.gate_lora.B'\n",
    "        W = f'model.layers.{i}.mlp.experts.gate_up_proj'\n",
    "        W = state_dict[W]\n",
    "        a = mapping[A].to(W.device)\n",
    "        b = mapping[B].to(W.device)\n",
    "\n",
    "        m = torch.matmul(a, b) * 2.0\n",
    "        W += m.to(W.dtype)\n",
    "        \n",
    "    A = f'model.layers.{i}.mlp.experts.down_lora.A'\n",
    "    if A in mapping:\n",
    "        B = f'model.layers.{i}.mlp.experts.down_lora.B'\n",
    "        W = f'model.layers.{i}.mlp.experts.down_proj'\n",
    "        W = state_dict[W]\n",
    "        a = mapping[A].to(W.device)\n",
    "        b = mapping[B].to(W.device)\n",
    "\n",
    "        m = torch.matmul(a, b) * 2.0\n",
    "        W += m.to(W.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae705a14-2d2d-4ff0-8925-7eebf4e0e23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 144/144 [00:00<00:00, 2249.84it/s]\n"
     ]
    }
   ],
   "source": [
    "keys_lora = [k.split('.lora')[0] for k in keys if '.lora' in k]\n",
    "keys_lora = sorted(list(set(keys_lora)))\n",
    "for k in tqdm(keys_lora):\n",
    "    k_ori = k.replace('_orig_mod.', '') + '.weight'\n",
    "    post_A = '.lora_A'\n",
    "    post_B = '.lora_B'\n",
    "    A = k + post_A\n",
    "    B = k + post_B\n",
    "    W = state_dict[k_ori]\n",
    "    A = mapping[A].to(W.device)\n",
    "    B = mapping[B].to(W.device)\n",
    "    m = torch.matmul(A.t(), B.t()) * 2.0\n",
    "    W += m.T.to(W.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a2914e-e7db-4856-b9c5-2b47703b46e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[200006,  17360, 200008,   3575,    553,  17554, 162016,     11,    261,\n",
       "           4410,   6439,   2359,  22203,    656,   7788,  17527,    558,  87447,\n",
       "         100594,     25,    220,   1323,     19,     12,   3218,    198,   6576,\n",
       "           3521,     25,    220,   1323,     21,     12,   2290,     12,   3114,\n",
       "            279,  30377,    289,     25,  14093,    279,      2,  13888,  18403,\n",
       "             25,   8450,     11,  49159,     11,   1721,     13,  21030,   2804,\n",
       "            413,   7360,    395,   1753,   3176,     13, 200007, 200006,  77944,\n",
       "         200008,      2,  68406,    279,   7127,     11,    481,   2075,    316,\n",
       "           2411,   5983,  23541,  41570,    306,   3849,    356,     11,   1934,\n",
       "            484,     11,   3006,    634,   1721,   6052,   3518, 126456, 172278,\n",
       "          12083,      3,    364, 200007, 200006,   1428, 200008,    198,  75908,\n",
       "            422,  15598,  32777,  11211,    280,     11,  22732,    516,  26322,\n",
       "         136404,  90461,     11,  85492,   6157,  10741,   7989,    272,    970,\n",
       "          15207,  11489,    364,    399,  12864,    849,   1610, 134697,    198,\n",
       "         200007, 200006, 173781]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\n",
    "\n",
    "terjemah ke kedah\n",
    "\"\"\"\n",
    "\n",
    "system = 'First, you try to think step-by-step in {{lang}}, after that, put your final answer within $\\\\boxed{}$.'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system.replace('{{lang}}', 'malay')},\n",
    "    {\"role\": \"user\", \"content\": q},\n",
    "]\n",
    "\n",
    "row = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "input_ids = tokenizer(row, add_special_tokens = False, return_tensors = 'pt').to(model.device)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7beed7f4-2d18-437e-9ac8-86071c01ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1109d6-d13d-425e-be8b-fa03cce5e1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2026-01-09\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "First, you try to think step-by-step in malay, after that, put your final answer within $\\boxed{}$.\n",
      "\n",
      "<|end|><|start|>user<|message|>\n",
      "Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\n",
      "\n",
      "terjemah ke kedah\n",
      "<|end|><|start|>assistant<|channel|>analysis<|message|>Baiklah, saya akan menerangkan dengan sangat terperinci dan panjang bagaimana ayat Bahasa Melayu standard di atas, **\"Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\"**, ditulis semula dalam dialek Kedah. Saya akan menerangkan setiap perkataan, perubahan bunyi, struktur ayat, dan nuansa budaya yang berkaitan supaya anda dapat memahami proses transformasi ayat ini dengan sangat mendalam.\n",
      "\n",
      "## **Langkah 1: Memahami Konteks dan Makna Ayat Standard**\n",
      "\n",
      "Ayat ini menggambarkan seseorang kanak-kanak yang sangat nakal atau jahil, sangat cepat melaksanakan sesuatu apabila diberi peluang, dan menggunakan duit syiling untuk sesuatu tujuan, mungkin untuk membeli sesuatu barang atau melakukan sesuatu yang tidak baik.\n",
      "\n",
      "### **Analisis Ayat:**\n",
      "\n",
      "* **Budak itu** -> Kanak-kanak yang dirujuk.\n",
      "* **Sangat nakal** -> Sangat tidak baik atau jahil.\n",
      "* **Pantang orang leka sedikit** -> Sekiranya diberi peluang sedikit-sikit.\n",
      "* **Duit syiling pun dikebasnya** -> Begitu juga duit syiling yang dimilikinya.\n",
      "\n",
      "## **Langkah 2: Terjemahan \"Budak itu\" -> \"Budok tu\"**\n",
      "\n",
      "* **Budak** -> **Budok**:\n",
      "  * Dalam dialek Kedah, bunyi 'a' sering diganti dengan 'o', jadi \"budak\" menjadi \"budok\".\n",
      "* **Itu** -> **Tu**:\n",
      "  * Dialek Kedah cenderung menggunakan \"tu\" untuk menunjukkan sesuatu yang dekat atau baru disebut.\n",
      "\n",
      "## **Langkah 3: Terjemahan \"Sangat nakal\" -> \"Banyak tarik\"**\n",
      "\n",
      "* **Sangat** -> **Banyak**:\n",
      "  * \"Sangat\" dalam dialek Kedah boleh ditukar kepada \"banyak\" untuk menunjukkan tahap yang tinggi atau banyak.\n",
      "* **Nakal** "
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 1024, \n",
    "    \"do_sample\": True, \n",
    "    \"temperature\": 0.9, \n",
    "    \"top_p\": None, \n",
    "    \"top_k\": None,\n",
    "    \"streamer\": streamer,\n",
    "}\n",
    "\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
