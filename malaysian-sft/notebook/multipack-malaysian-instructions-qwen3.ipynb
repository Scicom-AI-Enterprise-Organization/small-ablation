{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3239ee55-6f99-4329-ac6a-96b3eb636c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AddedToken\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "    'audio': 'str',\n",
    "    'text': 'str'\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'\n",
    "\n",
    "datasets = ['ayat_aktif_pasif', 'coding', 'malaysian_reasoning', 'meta_prompt', 'multiple_choice_qa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852be377-34dc-4b81-b570-1b191c7e0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-32B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4d6a51-e4eb-4a15-bd44-9860c623a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Scicom-intl/Malaysian-Instructions\", datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489547bd-9470-4331-bf6a-e86521781f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "ds = load_dataset(\"Scicom-intl/Malaysian-Instructions\", 'ayat_aktif_pasif')\n",
    "for i in range(len(ds['train'])):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': ds['train'][i]['input']},\n",
    "        {'role': 'assistant', 'content': ds['train'][i]['output']}\n",
    "    ]\n",
    "    data.append(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2969ad75-f7b2-48d2-823a-f6265d1861f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Scicom-intl/Malaysian-Instructions\", 'coding')\n",
    "for i in range(len(ds['train'])):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': ds['train'][i]['question']},\n",
    "        {'role': 'assistant', 'content': ds['train'][i]['answer']}\n",
    "    ]\n",
    "    data.append(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ae0785-f52f-45e9-86f7-b148401af6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Scicom-intl/Malaysian-Instructions\", 'malaysian_reasoning')\n",
    "for i in range(len(ds['train'])):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': ds['train'][i]['system']},\n",
    "        {'role': 'user', 'content': ds['train'][i]['input']},\n",
    "        {'role': 'assistant', 'content': ds['train'][i]['answer'], 'reasoning_content': ds['train'][i]['reasoning']}\n",
    "    ]\n",
    "    data.append(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e7442-6d9a-421b-b0e5-acd094b5397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Scicom-intl/Malaysian-Instructions\", 'meta_prompt')\n",
    "for i in range(len(ds['train'])):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': ds['train'][i]['input']},\n",
    "        {'role': 'assistant', 'content': ds['train'][i]['answer']}\n",
    "    ]\n",
    "    data.append(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e6a2a-3440-4b64-8b56-d4a7ffdd9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Scicom-intl/Malaysian-Instructions\", 'multiple_choice_qa')\n",
    "for i in range(len(ds['train'])):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': ds['train'][i]['question']},\n",
    "        {'role': 'assistant', 'content': ds['train'][i]['answer']}\n",
    "    ]\n",
    "    data.append(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb928a70-ff07-4fbd-833e-f4fbd2ab1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dae84-818a-455e-ba16-9135dea63d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tokenized-qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24122187-51dc-40c5-827e-8e90209dc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "        'audio': '',\n",
    "        'text': '',\n",
    "    }\n",
    "\n",
    "sequence_length = 1024 * 16\n",
    "def loop(files, block_size = sequence_length):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-qwen3/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "\n",
    "            t = tokenizer.apply_chat_template(row, tokenize=False)\n",
    "            outputs = tokenizer(t, add_special_tokens=False)\n",
    "            position = range(len(outputs['input_ids']))\n",
    "            length = len(outputs['input_ids'])\n",
    "\n",
    "            if length > block_size:\n",
    "                continue\n",
    "            \n",
    "            if count + length > block_size:\n",
    "                o = collator(temp, position_ids)\n",
    "                if o['input_ids'].shape[0] > 0:\n",
    "                    out.write(o)\n",
    "                temp = [outputs['input_ids']]\n",
    "                position_ids = [position]\n",
    "                count = length\n",
    "                \n",
    "            else:\n",
    "                temp.append(outputs['input_ids'])\n",
    "                position_ids.append(range(len(outputs['input_ids'])))\n",
    "                count += len(outputs['input_ids'])\n",
    "        \n",
    "        if len(temp):\n",
    "            o = collator(temp, position_ids)\n",
    "            if o['input_ids'].shape[0] > 0:\n",
    "                out.write(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357d1e9-ac80-4ab2-b3a2-60090105a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop((data[:100], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad2a01-bb7b-4d2c-9a60-f1723d67de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing(data, loop, cores = 20, returned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab680ec2-6e38-40a0-b459-4b89e42b6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "folders = sorted(glob('tokenized-qwen3/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79969f63-2f46-41da-ba46-cbe05c006899",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf multipacking-qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26954959-ca3c-4509-be13-9a682d1fbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with MDSWriter(out='multipacking-qwen3', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0b272-f278-4a35-b758-2de32e064c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LocalDataset('multipacking-qwen3')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4f7b0-2d94-4283-bf8d-c5178f7427e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866b352b-89ce-405e-a540-a539215d708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen3ForCausalLM\n",
    "\n",
    "model = Qwen3ForCausalLM.from_pretrained('Qwen/Qwen3-0.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444e9f75-ab24-40c8-a7a6-fbfebaf60340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169676db-e627-42cb-b3c4-7531a2b837b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_features = linear.in_features\n",
    "        out_features = linear.out_features\n",
    "        \n",
    "        device = self.linear.weight.device\n",
    "        dtype = self.linear.weight.dtype\n",
    "\n",
    "        self.lora_A = nn.ModuleDict({})\n",
    "        self.lora_B = nn.ModuleDict({})\n",
    "        \n",
    "        self.lora_A['e'] = nn.Linear(\n",
    "            in_features, r, bias=False, \n",
    "            device = device,\n",
    "            dtype = torch.float32,\n",
    "        )\n",
    "        self.lora_B['e'] = nn.Linear(\n",
    "            r, out_features, bias=False, \n",
    "            device = device,\n",
    "            dtype = torch.float32,\n",
    "        )\n",
    "\n",
    "        for param in self.lora_A['e'].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.lora_B['e'].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py#L260\n",
    "        init.kaiming_uniform_(self.lora_A['e'].weight, a=math.sqrt(5))\n",
    "        init.zeros_(self.lora_B['e'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        lora_update = self.lora_B['e'](self.lora_A['e'](x.to(self.lora_A['e'].weight.dtype))) * self.scaling\n",
    "        return out + lora_update.to(x.dtype)\n",
    "\n",
    "selected = [\n",
    "    \"q_proj\", \n",
    "    \"k_proj\", \n",
    "    \"v_proj\", \n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "for name, module in model.named_modules():\n",
    "    for child_name, child in module.named_children():\n",
    "        if len(child_name) and any([a in child_name for a in selected]) and isinstance(child, nn.Linear):\n",
    "            lora = LinearLoRA(child, r=128, alpha=256)\n",
    "            setattr(module, child_name, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcff6e07-9fad-4174-a618-fb57bd4b8c5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m dp_size = \u001b[32m4\u001b[39m\n\u001b[32m      5\u001b[39m device_type = torch.accelerator.current_accelerator().type\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m device_mesh = \u001b[43minit_device_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdp_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_dim_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/device_mesh.py:1046\u001b[39m, in \u001b[36minit_device_mesh\u001b[39m\u001b[34m(device_type, mesh_shape, mesh_dim_names)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1045\u001b[39m     mesh = torch.arange(math.prod(mesh_shape), dtype=torch.int).view(mesh_shape)\n\u001b[32m-> \u001b[39m\u001b[32m1046\u001b[39m device_mesh = \u001b[43mDeviceMesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmesh_dim_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmesh_dim_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m device_mesh\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/device_mesh.py:462\u001b[39m, in \u001b[36mDeviceMesh.__init__\u001b[39m\u001b[34m(self, device_type, mesh, mesh_dim_names, _init_backend)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_type != \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# always try to create default (world) pg, even if it is not initialized\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m# already. The world pg is used for device mesh identity (rank) on each\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m# process (we need to know if the current global rank is in the mesh or not).\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _init_backend:\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_world_group_and_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m         \u001b[38;5;28mself\u001b[39m._init_process_groups()\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized() \u001b[38;5;129;01mand\u001b[39;00m get_backend() == \u001b[33m\"\u001b[39m\u001b[33mthreaded\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/device_mesh.py:480\u001b[39m, in \u001b[36mDeviceMesh._setup_world_group_and_device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;66;03m# TODO: think about how to allow pg options to be passed to world group\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# or mesh dimension groups\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m default_initialized:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m world_size = get_world_size()\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mesh.numel() > world_size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:81\u001b[39m, in \u001b[36m_exception_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m     83\u001b[39m         msg_dict = _get_msg_dict(func.\u001b[34m__name__\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:95\u001b[39m, in \u001b[36m_time_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _WaitCounter(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpytorch.wait_counter.c10d.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m).guard():\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         func_return = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func_return\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1757\u001b[39m, in \u001b[36minit_process_group\u001b[39m\u001b[34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1754\u001b[39m     rendezvous_iterator = rendezvous(\n\u001b[32m   1755\u001b[39m         not_none(init_method), rank, world_size, timeout=timeout\n\u001b[32m   1756\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m     store, rank, world_size = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1758\u001b[39m     store.set_timeout(timeout)\n\u001b[32m   1760\u001b[39m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[32m   1761\u001b[39m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/rendezvous.py:267\u001b[39m, in \u001b[36m_env_rendezvous_handler\u001b[39m\u001b[34m(url, timeout, **kwargs)\u001b[39m\n\u001b[32m    265\u001b[39m     rank = \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[33m\"\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     rank = \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRANK\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mworld_size\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[32m    270\u001b[39m     world_size = \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[33m\"\u001b[39m\u001b[33mworld_size\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/distributed/rendezvous.py:252\u001b[39m, in \u001b[36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[39m\u001b[34m(env_var)\u001b[39m\n\u001b[32m    250\u001b[39m env_val = os.environ.get(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[31mValueError\u001b[39m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "\n",
    "dp_size = 4\n",
    "device_type = torch.accelerator.current_accelerator().type\n",
    "device_mesh = init_device_mesh(device_type, (dp_size,), mesh_dim_names=(\"dp\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5338988-dcb7-429f-8095-db921a06575f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
