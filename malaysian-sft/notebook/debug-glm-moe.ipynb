{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bd6666-7cc8-4708-bfc3-e38860b17ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 80659.69it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 88701.29it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 77060.70it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 75475.91it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74518.09it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 71089.90it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 65682.61it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 69245.58it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 101944.89it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 118387.61it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 98855.65it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 61680.94it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 22724.56it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 78713.48it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 60411.79it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 90061.74it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 55.89it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 58.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 57.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 56.45it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 56.95it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 57.17it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 56.94it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 55.90it/s]\n",
      "16751it [01:24, 198.26it/s]\n",
      "16751it [01:37, 171.53it/s]\n",
      "16446it [01:38, 204.87it/s]\n",
      "16751it [01:39, 169.14it/s]\n",
      "16751it [01:39, 169.11it/s]\n",
      "16751it [01:39, 168.98it/s]\n",
      "16751it [01:39, 168.94it/s]\n",
      "16751it [01:39, 168.87it/s]\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/root/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "from jupytertracerviz import init_multigpus_repl, multigpus\n",
    "\n",
    "init_multigpus_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e7047b-3615-4a14-a16d-f370e683c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.tensor import distribute_tensor\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import fully_shard, MixedPrecisionPolicy, CPUOffloadPolicy\n",
    "from functools import partial\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing,\n",
    "    checkpoint_wrapper,\n",
    ")\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Glm4MoeForCausalLM,\n",
    ")\n",
    "from transformers.models.glm4_moe.modeling_glm4_moe import (\n",
    "    Glm4MoeMLP,\n",
    "    Glm4MoeDecoderLayer,\n",
    "    Glm4MoeTopkRouter,\n",
    ")\n",
    "from transformers.models.glm4_moe import modeling_glm4_moe\n",
    "from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "from streaming import LocalDataset\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "import numpy as np\n",
    "\n",
    "class Model(Glm4MoeForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.loss = LigerFusedLinearCrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        position_ids=None, \n",
    "        labels=None, \n",
    "        num_items_in_batch=None, \n",
    "        logits_to_keep=0,\n",
    "        output_router_logits=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super_out = self.model.forward(\n",
    "            input_ids = input_ids,\n",
    "            position_ids = position_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            output_router_logits=output_router_logits,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if labels is not None:\n",
    "            embeddings = super_out.last_hidden_state\n",
    "            slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "            embeddings = embeddings[:, slice_indices, :]\n",
    "            embeddings = embeddings[:,:-1].reshape(-1, embeddings.shape[-1])\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            labels = labels.reshape(-1)\n",
    "            \n",
    "            loss = self.loss(self.lm_head.weight, embeddings, labels)\n",
    "            print(self.lm_head.weight.shape, embeddings.shape, labels.shape, loss)\n",
    "            num_items_in_batch = num_items_in_batch.to(loss.device)\n",
    "\n",
    "            loss = loss / num_items_in_batch\n",
    "            return {'loss': loss}\n",
    "        return super_out\n",
    "\n",
    "class Glm4MoeMoEExpertParallel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device_mesh = self.config.device_mesh\n",
    "        self.world_size = self.device_mesh.size()\n",
    "        self.rank = self.device_mesh.get_local_rank()\n",
    "        \n",
    "        self.total_experts = config.n_routed_experts\n",
    "        self.experts_per_rank = self.total_experts // self.world_size\n",
    "        assert self.total_experts % self.world_size == 0, \\\n",
    "            f\"n_routed_experts ({self.total_experts}) must be divisible by world_size ({self.world_size})\"\n",
    "        \n",
    "        self.local_expert_start = self.rank * self.experts_per_rank\n",
    "        self.local_expert_end = (self.rank + 1) * self.experts_per_rank\n",
    "        self._is_sharded = False\n",
    "\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                Glm4MoeMLP(config, intermediate_size=config.moe_intermediate_size)\n",
    "                for _ in range(config.n_routed_experts)\n",
    "            ]\n",
    "        )\n",
    "        self.gate = Glm4MoeTopkRouter(config)\n",
    "        self.shared_experts = Glm4MoeMLP(\n",
    "            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n",
    "        )\n",
    "\n",
    "    def shard_experts(self):\n",
    "        if self._is_sharded:\n",
    "            return\n",
    "\n",
    "        local_experts = nn.ModuleList([\n",
    "            self.experts[i] for i in range(self.local_expert_start, self.local_expert_end)\n",
    "        ])\n",
    "        del self.experts\n",
    "        torch.cuda.empty_cache()\n",
    "        self.experts = local_experts\n",
    "        self._is_sharded = True\n",
    "\n",
    "    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n",
    "        if self._is_sharded:\n",
    "            return self._moe_sharded(hidden_states, topk_indices, topk_weights)\n",
    "        else:\n",
    "            return self._moe_local(hidden_states, topk_indices, topk_weights)\n",
    "    \n",
    "    def _moe_local(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n",
    "        hidden_dim = hidden_states.shape[-1]\n",
    "        final_hidden_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype)\n",
    "        \n",
    "        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts)).permute(2, 1, 0)\n",
    "        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero().squeeze(-1)\n",
    "        \n",
    "        for expert_idx in expert_hit:\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "    \n",
    "            current_state = hidden_states[top_x]\n",
    "            current_hidden_states = expert_layer(current_state) * topk_weights[top_x, idx, None]\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        \n",
    "        return final_hidden_states.type(hidden_states.dtype)\n",
    "    \n",
    "    def _moe_sharded(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n",
    "        final_hidden_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype)\n",
    "        \n",
    "        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=self.total_experts).permute(2, 1, 0)\n",
    "        \n",
    "        for local_expert_idx in range(self.experts_per_rank):\n",
    "            global_expert_idx = self.local_expert_start + local_expert_idx\n",
    "            expert_layer = self.experts[local_expert_idx]\n",
    "            \n",
    "            idx, top_x = torch.where(expert_mask[global_expert_idx])\n",
    "            \n",
    "            if top_x.numel() > 0:\n",
    "                current_state = hidden_states[top_x]\n",
    "                current_hidden_states = expert_layer(current_state) * topk_weights[top_x, idx, None]\n",
    "                final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "            else:\n",
    "                dummy = expert_layer(hidden_states[:1]) * 0\n",
    "                final_hidden_states = final_hidden_states + dummy.sum() * 0\n",
    "                \n",
    "        torch.distributed.all_reduce(final_hidden_states, group=self.device_mesh.get_group())\n",
    "        return final_hidden_states.type(hidden_states.dtype)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        residuals = hidden_states\n",
    "        orig_shape = hidden_states.shape\n",
    "        topk_indices, topk_weights = self.gate(hidden_states)\n",
    "        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n",
    "        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n",
    "        hidden_states = hidden_states + self.shared_experts(residuals)\n",
    "        return hidden_states\n",
    "\n",
    "modeling_glm4_moe.Glm4MoeMoE = Glm4MoeMoEExpertParallel\n",
    "\n",
    "def check_fn(module):\n",
    "    return isinstance(module, (Glm4MoeDecoderLayer, Glm4MoeMLP, Glm4MoeMoEExpertParallel))\n",
    "\n",
    "non_reentrant_wrapper = partial(\n",
    "    checkpoint_wrapper,\n",
    "    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n",
    ")\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.dataset = LocalDataset(local=folder)\n",
    "        self.sequence_length = 16384\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        data.pop('audio', None)\n",
    "        data.pop('text', None)\n",
    "        data.pop('token_type_ids', None)\n",
    "\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].astype(np.int64)\n",
    "\n",
    "        data['labels'] = data['input_ids'].copy()\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def collator(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    position_ids = [b['position_ids'] for b in batch]\n",
    "    labels = [b['labels'] for b in batch]\n",
    "    attention_mask = [b['attention_mask'] for b in batch]\n",
    "    input_ids = np.concatenate(input_ids)\n",
    "    position_ids = np.concatenate(position_ids)\n",
    "    labels = np.concatenate(labels)\n",
    "    query_lens = np.concatenate(attention_mask)\n",
    "    cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "    max_cumsum = int(np.max(cumsum))\n",
    "    cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    cu_seq_lens_k = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    max_seqlen_q = int(np.max(query_lens))\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[None],\n",
    "        'position_ids': torch.tensor(position_ids)[None],\n",
    "        'labels': torch.tensor(labels)[None],\n",
    "        'cu_seq_lens_q': cu_seq_lens_q,\n",
    "        'cu_seq_lens_k': cu_seq_lens_k,\n",
    "        'max_length_q': max_seqlen_q,\n",
    "        'max_length_k': max_seqlen_q\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f261bc0-d738-4a62-a32a-6ba307bdf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_features = linear.in_features\n",
    "        out_features = linear.out_features\n",
    "        \n",
    "        device = self.linear.weight.device\n",
    "        dtype = self.linear.weight.dtype\n",
    "\n",
    "        self.lora_A = nn.ModuleDict({})\n",
    "        self.lora_B = nn.ModuleDict({})\n",
    "        \n",
    "        self.lora_A['e'] = nn.Linear(\n",
    "            in_features, r, bias=False, \n",
    "            device = device,\n",
    "            dtype = torch.float32,\n",
    "        )\n",
    "        self.lora_B['e'] = nn.Linear(\n",
    "            r, out_features, bias=False, \n",
    "            device = device,\n",
    "            dtype = torch.float32,\n",
    "        )\n",
    "\n",
    "        for param in self.lora_A['e'].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.lora_B['e'].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py#L260\n",
    "        init.kaiming_uniform_(self.lora_A['e'].weight, a=math.sqrt(5))\n",
    "        init.zeros_(self.lora_B['e'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        lora_update = self.lora_B['e'](self.lora_A['e'](x.to(self.lora_A['e'].weight.dtype))) * self.scaling\n",
    "        out = out + lora_update.to(x.dtype)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4e25e5-927c-42ef-967b-9b0c8b0987bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "import os\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "world_size = int(os.environ['WORLD_SIZE'])\n",
    "device_type = torch.accelerator.current_accelerator()\n",
    "device = torch.device(f\"{device_type}:{rank}\")\n",
    "device_mesh = init_device_mesh(device_type.type, (world_size,), mesh_dim_names=(\"dp\",))\n",
    "tp_mesh = device_mesh[\"dp\"]\n",
    "dp_mesh = device_mesh[\"dp\"]\n",
    "dp_rank = dp_mesh.get_local_rank()\n",
    "dp_world_size = dp_mesh.size()\n",
    "\n",
    "model_name = \"ramdisk/GLM-4.5-Air\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.device_mesh = tp_mesh\n",
    "\n",
    "model = Model.from_pretrained(\n",
    "    model_name, \n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727c0b99-c6a5-407e-b947-553a9c4ccf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Glm4MoeMoEExpertParallel):\n",
    "        module.shard_experts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc718414-3220-4beb-8213-e43f0033fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "selected = [\n",
    "    \"q_proj\", \n",
    "    \"k_proj\", \n",
    "    \"v_proj\", \n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "rank_lora = 256\n",
    "alpha_lora = 512\n",
    "top_k = model.config.num_experts_per_tok\n",
    "r = rank_lora // top_k\n",
    "alpha = alpha_lora // top_k\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    for child_name, child in module.named_children():\n",
    "        if len(child_name) and any([a in child_name for a in selected]) and isinstance(child, nn.Linear):\n",
    "            if 'mlp.experts' in name:\n",
    "                selected_rank = r\n",
    "                selected_alpha = alpha\n",
    "            else:\n",
    "                selected_rank = rank_lora\n",
    "                selected_alpha = alpha_lora\n",
    "\n",
    "            lora = LinearLoRA(child, r=selected_rank, alpha=selected_alpha)\n",
    "            setattr(module, child_name, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd18978-d805-4bfd-b121-b55057ac0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "ignored_params = set()\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Glm4MoeMoEExpertParallel):\n",
    "        for param in module.experts.parameters():\n",
    "            ignored_params.add(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92c885b-caa4-4a85-bcbe-9dd82b028718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "lora_mp_policy = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.float32,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    for child_name, child in module.named_children():\n",
    "        if '.lora' in name and isinstance(child, nn.Linear):\n",
    "            fully_shard(child, mp_policy=lora_mp_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf708ebe-989a-4556-b8eb-f03eef23d89c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "fsdp_kwargs = {}\n",
    "fsdp_kwargs[\"mp_policy\"] = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "fsdp_kwargs[\"mesh\"] = dp_mesh\n",
    "fsdp_kwargs[\"ignored_params\"] = ignored_params\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Glm4MoeDecoderLayer):\n",
    "        fully_shard(module, **fsdp_kwargs)\n",
    "\n",
    "fully_shard(model, **fsdp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a6f1a5-b839-4123-b391-dfc856e56d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Glm4MoeMoEExpertParallel):\n",
    "        module.experts = module.experts.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae61c796-2de0-4667-9108-6af5e8991b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "apply_activation_checkpointing(\n",
    "    model,\n",
    "    checkpoint_wrapper_fn=non_reentrant_wrapper,\n",
    "    check_fn=check_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86ca3b0-5404-4d20-ab09-66a08819bc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%multigpus\n",
    "\n",
    "# dataset = Dataset('multipacking-glm')\n",
    "\n",
    "# b = [dataset[0], dataset[1]]\n",
    "# b = collator(b)\n",
    "# with torch.no_grad():\n",
    "#     for k in b.keys():\n",
    "#         if isinstance(b[k], torch.Tensor):\n",
    "#             b[k] = b[k].to(model.device, non_blocking=True)\n",
    "\n",
    "#     valid_tokens = (b['labels'] != -100).sum().item() * dp_world_size\n",
    "#     b['num_items_in_batch'] = torch.tensor(valid_tokens)\n",
    "#     out = model(**b, use_cache=False)\n",
    "\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f191abcf-b079-4350-a8ac-77d536c885d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32e96744-3c56-4769-abd6-ec6f9616c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe3c4c2-1754-4d77-8a00-5bf1feb4728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "dataset = Dataset('multipacking-glm')\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_world_size,\n",
    "    rank=dp_rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=5,\n",
    "    prefetch_factor=5,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "iter_train_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ca4214-8166-423f-860b-b6211bf76d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%multigpus\n",
    "\n",
    "# strings = [\n",
    "#     \"hello what is your name\",\n",
    "#     \"testing testing 123\",\n",
    "#     'Perdana Menteri, Datuk Seri Anwar Ibrahim menjelaskan ketidakhadirannya ke sidang Dewan Rakyat pagi tadi kerana menghadiri sesi taklimat khas Majlis Raja-Raja Melayu di Istana Negara yang membincangkan pelbagai isu semasa dan urusan rasmi negara.'\n",
    "# ]\n",
    "# mapping = {}\n",
    "# for s in strings:\n",
    "#     messages = [{\"role\": \"user\", \"content\": s}]\n",
    "#     inputs = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=True,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_dict=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     inputs.pop('token_type_ids')\n",
    "#     with torch.no_grad():\n",
    "#         inputs = inputs.to(device)\n",
    "#         o = model(**inputs, use_cache=False)\n",
    "#         l = model.lm_head(o.last_hidden_state)\n",
    "#         mapping[s] = l.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33adc6a5-cfdf-4dad-ac0d-c5747cfdfff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%multigpus\n",
    "\n",
    "# here I already verified\n",
    "# mapping_logits = torch.load('moe-logits.pkl')\n",
    "# for k, v in mapping_logits.items():\n",
    "#     print(k, (v - mapping[k]).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98ad6942-f7ff-4e91-8cd6-d2e19a17c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 5] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:5',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 5] done forward\n",
      "[GPU 1] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:1',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 1] done forward\n",
      "[GPU 0] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 0] done forward\n",
      "[GPU 6] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:6',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 7] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:7',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 6] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 2] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:2',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 2] done forward\n",
      "[GPU 3] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:3',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 3] done forward\n",
      "[GPU 4] torch.Size([151552, 4096]) torch.Size([32697, 4096]) torch.Size([32697]) tensor(136561.2344, device='cuda:4',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 4] done forward\n",
      "[GPU 7] 0 tensor(4.1764, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 5] 0 tensor(4.1764, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 0 tensor(4.1764, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 6] 0 tensor(4.1764, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 0 tensor(4.1764, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 1] 0 tensor(4.1764, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 0 tensor(4.1764, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 0 tensor(4.1764, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.5221, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 5] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:5',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 2] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:2',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 6] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:6',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 3] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:3',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 0] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 7] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:7',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 1] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:1',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 4] torch.Size([151552, 4096]) torch.Size([32683, 4096]) torch.Size([32683]) tensor(91878.3359, device='cuda:4',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 5] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 5] 1 tensor(2.8111, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 1 tensor(2.8111, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 6] 1 tensor(2.8111, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 1] 1 tensor(2.8111, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 1 tensor(2.8111, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 1 tensor(2.8111, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 1 tensor(2.8111, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 1 tensor(2.8111, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.3514, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 5] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:5',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 0] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 6] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:6',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 7] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:7',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 2] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:2',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 1] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:1',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 3] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:3',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 4] torch.Size([151552, 4096]) torch.Size([32645, 4096]) torch.Size([32645]) tensor(61792.6016, device='cuda:4',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 6] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 5] 2 tensor(1.8928, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 6] 2 tensor(1.8928, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 2 tensor(1.8928, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 2 tensor(1.8928, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 2 tensor(1.8928, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 2 tensor(1.8928, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 2 tensor(1.8928, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 1] 2 tensor(1.8928, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.2366, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 5] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:5',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 6] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:6',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 3] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:3',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 7] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:7',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 2] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:2',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 1] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:1',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 0] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 4] torch.Size([151552, 4096]) torch.Size([32703, 4096]) torch.Size([32703]) tensor(51578.2617, device='cuda:4',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 5] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 6] 3 tensor(1.5771, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 3 tensor(1.5771, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 5] 3 tensor(1.5771, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 3 tensor(1.5771, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 3 tensor(1.5771, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 3 tensor(1.5771, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 3 tensor(1.5771, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 1] 3 tensor(1.5771, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.1971, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 6] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:6',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 3] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:3',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 2] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:2',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 5] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:5',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 7] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:7',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 1] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:1',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 0] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 4] torch.Size([151552, 4096]) torch.Size([32722, 4096]) torch.Size([32722]) tensor(46024.2422, device='cuda:4',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "[GPU 3] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 6] 4 tensor(1.4065, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 5] 4 tensor(1.4065, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 4 tensor(1.4065, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 4 tensor(1.4065, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 4 tensor(1.4065, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 1] 4 tensor(1.4065, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 4 tensor(1.4065, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 4 tensor(1.4065, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.1758, device='cuda:7', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_grad_norm_(parameters, max_norm, norm_type=2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    else:\n",
    "        parameters = list(parameters)\n",
    "    \n",
    "    grads = [p.grad for p in parameters if p.grad is not None]\n",
    "    \n",
    "    norms = []\n",
    "    for grad in grads:\n",
    "        if hasattr(grad, 'full_tensor'):  # DTensor\n",
    "            grad_full = grad.full_tensor()\n",
    "        else:\n",
    "            grad_full = grad\n",
    "        \n",
    "        if norm_type == float('inf'):\n",
    "            norms.append(grad_full.abs().max())\n",
    "        else:\n",
    "            norms.append(grad_full.norm(norm_type))\n",
    "    \n",
    "    if len(norms) == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "    total_norm = torch.stack(norms).norm(norm_type)\n",
    "    \n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
    "    \n",
    "    for grad in grads:\n",
    "        if hasattr(grad, 'full_tensor'):  # DTensor\n",
    "            grad.mul_(clip_coef_clamped)\n",
    "        else:\n",
    "            grad.mul_(clip_coef_clamped)\n",
    "    \n",
    "    return total_norm\n",
    "    \n",
    "for i in range(5):\n",
    "    total_tokens = 0\n",
    "    b = next(iter_train_loader)\n",
    "    for k in b.keys():\n",
    "        if isinstance(b[k], torch.Tensor):\n",
    "            b[k] = b[k].to(device, non_blocking=True)\n",
    "\n",
    "    valid_tokens = (b['labels'] != -100).sum().item()\n",
    "    total_tokens += valid_tokens\n",
    "    token_tensor = torch.tensor([total_tokens], dtype=torch.long, device=device)\n",
    "    dp_group = dp_mesh.get_group()\n",
    "    dist.all_reduce(token_tensor, op=dist.ReduceOp.SUM, group=dp_group)\n",
    "    global_total_tokens = token_tensor.item()\n",
    "    b['num_items_in_batch'] = torch.tensor(global_total_tokens)\n",
    "    out = model(**b, use_cache=False)\n",
    "    loss = out[\"loss\"] * dp_world_size\n",
    "    print('done forward')\n",
    "    loss.backward()\n",
    "    grad_norm = clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    print(i, loss, out[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
