{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bccaaf1d-95d9-4b09-b4cb-76cd6d52dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 78503.02it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 79137.81it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 83173.17it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 79137.81it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 57009.96it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 78713.48it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 30872.90it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 69409.29it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 63140.06it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 69409.29it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 38479.85it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 67806.30it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 52522.59it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 17311.40it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 63276.14it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 16266.00it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 305.07it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 304.83it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 387.09it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 442.64it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 440.55it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 445.48it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 378.43it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 372.33it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 95948.13it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 21845.33it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 72137.91it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 92911.80it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 96579.37it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 65244.73it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 96262.71it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 97218.97it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 82011.53it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 85101.82it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 30647.32it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 40721.40it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 30051.31it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 44893.16it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 56679.78it/s]\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 77672.30it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 394.22it/s]\n",
      "824it [00:00, 1004.98it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 396.55it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 389.29it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 358.78it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 275.28it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 315.37it/s]\n",
      "824it [00:00, 1074.93it/s]\n",
      "824it [00:00, 1025.59it/s]\n",
      "824it [00:00, 1017.23it/s]\n",
      "824it [00:00, 1112.29it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 287.42it/s]\n",
      "824it [00:00, 1064.73it/s]\n",
      "Loading checkpoint shards: 100%|███████████████| 13/13 [00:00<00:00, 359.45it/s]\n",
      "824it [00:00, 1194.82it/s]\n",
      "824it [00:00, 1164.57it/s]\n",
      "968it [00:04, 209.22it/s]\n",
      "968it [00:04, 219.48it/s]\n",
      "968it [00:04, 216.99it/s]\n",
      "968it [00:04, 221.43it/s]\n",
      "968it [00:04, 226.11it/s]\n",
      "968it [00:04, 199.41it/s]\n",
      "968it [00:04, 227.46it/s]\n",
      "968it [00:04, 200.49it/s]\n",
      "968it [00:15, 62.16it/s]\n",
      "968it [00:15, 62.07it/s]\n",
      "968it [00:15, 62.06it/s]\n",
      "968it [00:15, 62.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "968it [00:16, 58.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from jupytertracerviz import init_multigpus_repl, multigpus\n",
    "\n",
    "init_multigpus_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f986f7-184e-4e88-b2ea-9071bacd34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "import torch\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing,\n",
    "    checkpoint_wrapper,\n",
    ")\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.tensor import distribute_tensor\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import fully_shard, MixedPrecisionPolicy, CPUOffloadPolicy\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Qwen3MoeForCausalLM,\n",
    ")\n",
    "from transformers.models.qwen3_moe.modeling_qwen3_moe import (\n",
    "    Qwen3MoeMLP,\n",
    "    Qwen3MoeDecoderLayer,\n",
    "    load_balancing_loss_func,\n",
    "    ACT2FN,\n",
    ")\n",
    "from transformers.models.qwen3_moe import modeling_qwen3_moe\n",
    "from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "from streaming import LocalDataset\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, folder, sequence_length=16384):\n",
    "        self.dataset = LocalDataset(local=folder)\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        data.pop('audio', None)\n",
    "        data.pop('text', None)\n",
    "        data.pop('token_type_ids', None)\n",
    "\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].astype(np.int64)\n",
    "\n",
    "        data['labels'] = data['input_ids'].copy()\n",
    "        attention_mask_sum = data['attention_mask'].sum()\n",
    "        \n",
    "        if attention_mask_sum < self.sequence_length:\n",
    "            balance = self.sequence_length - attention_mask_sum\n",
    "            data['input_ids'] = np.concatenate([data['input_ids'], np.array([151329] * balance)])\n",
    "            data['position_ids'] = np.concatenate([data['position_ids'], np.array([0] * balance)])\n",
    "            data['labels'] = np.concatenate([data['labels'], np.array([-100] * balance)])\n",
    "            data['attention_mask'] = np.concatenate([data['attention_mask'], np.array([balance])])\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def collator(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    position_ids = [b['position_ids'] for b in batch]\n",
    "    labels = [b['labels'] for b in batch]\n",
    "    attention_mask = [b['attention_mask'] for b in batch]\n",
    "    input_ids = np.concatenate(input_ids)\n",
    "    position_ids = np.concatenate(position_ids)\n",
    "    labels = np.concatenate(labels)\n",
    "    query_lens = np.concatenate(attention_mask)\n",
    "    cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "    max_cumsum = int(np.max(cumsum))\n",
    "    cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    cu_seq_lens_k = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    max_seqlen_q = int(np.max(query_lens))\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[None],\n",
    "        'position_ids': torch.tensor(position_ids)[None],\n",
    "        'labels': torch.tensor(labels)[None],\n",
    "        'cu_seq_lens_q': cu_seq_lens_q,\n",
    "        'cu_seq_lens_k': cu_seq_lens_k,\n",
    "        'max_length_q': max_seqlen_q,\n",
    "        'max_length_k': max_seqlen_q\n",
    "    }\n",
    "\n",
    "class Model(Qwen3MoeForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.loss = LigerFusedLinearCrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        position_ids=None, \n",
    "        labels=None, \n",
    "        num_items_in_batch=None, \n",
    "        logits_to_keep=0,\n",
    "        output_router_logits=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "        super_out = self.model.forward(\n",
    "            input_ids = input_ids,\n",
    "            position_ids = position_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            output_router_logits=output_router_logits,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if labels is not None:\n",
    "            embeddings = super_out.last_hidden_state\n",
    "            slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "            embeddings = embeddings[:, slice_indices, :]\n",
    "            embeddings = embeddings[:,:-1].reshape(-1, embeddings.shape[-1])\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            labels = labels.reshape(-1)\n",
    "            \n",
    "            loss = self.loss(self.lm_head.weight, embeddings, labels)\n",
    "            num_items_in_batch = num_items_in_batch.to(loss.device)\n",
    "\n",
    "            loss = loss / num_items_in_batch\n",
    "            if output_router_logits:\n",
    "                aux_loss = load_balancing_loss_func(\n",
    "                    super_out.router_logits,\n",
    "                    self.num_experts,\n",
    "                    self.num_experts_per_tok,\n",
    "                    attention_mask,\n",
    "                )\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)\n",
    "            return {'loss': loss}\n",
    "        return super_out\n",
    "\n",
    "class ExpertLoRAWeights(nn.Module):\n",
    "    \"\"\"Wrapper to make expert LoRA weights more FSDP-friendly\"\"\"\n",
    "    def __init__(self, num_experts, in_dim, out_dim, r, dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.zeros(num_experts, in_dim, r, dtype=dtype))\n",
    "        self.B = nn.Parameter(torch.zeros(num_experts, r, out_dim, dtype=dtype))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "\n",
    "class Qwen3MoeSparseMoeBlockParallel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.norm_topk_prob = config.norm_topk_prob\n",
    "\n",
    "        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n",
    "        self.gate_proj = nn.Parameter(torch.zeros(self.num_experts, config.hidden_size, config.moe_intermediate_size))\n",
    "        self.up_proj = nn.Parameter(torch.zeros(self.num_experts, config.hidden_size, config.moe_intermediate_size))\n",
    "        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, config.moe_intermediate_size, config.hidden_size))\n",
    "        self._is_stacked = False\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "    \n",
    "    def apply_lora_stack(self, r, alpha):\n",
    "        if self._is_stacked:\n",
    "            return\n",
    "\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.alpha = alpha / r\n",
    "        \n",
    "        self._is_stacked = True\n",
    "\n",
    "        self.gate_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.gate_proj.shape[1], self.gate_proj.shape[2], r\n",
    "        )\n",
    "        self.up_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.up_proj.shape[1], self.up_proj.shape[2], r\n",
    "        )\n",
    "        self.down_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.down_proj.shape[1], self.down_proj.shape[2], r\n",
    "        )\n",
    "\n",
    "    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n",
    "        M = hidden_states.shape[0]\n",
    "        hidden_dim = hidden_states.shape[-1]\n",
    "\n",
    "        sort_indices = topk_indices.view(-1).argsort()  # (M * topk,)\n",
    "        sorted_pos = sort_indices // self.top_k\n",
    "        grouped_inputs = hidden_states[sorted_pos]  # (M * topk, hidden_dim)\n",
    "\n",
    "        experts_count = topk_indices.view(-1).bincount(minlength=self.num_experts)\n",
    "        cu_experts_count = experts_count.cumsum(dim=0).to(torch.int32)\n",
    "\n",
    "        gate_out = torch._grouped_mm(\n",
    "            grouped_inputs,\n",
    "            self.gate_proj,\n",
    "            cu_experts_count,\n",
    "        )\n",
    "        if self.gate_lora is not None:\n",
    "            gate_out_lora_A = torch._grouped_mm(\n",
    "                grouped_inputs,\n",
    "                self.gate_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            gate_out_lora_B = torch._grouped_mm(\n",
    "                gate_out_lora_A,\n",
    "                self.gate_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            gate_out = gate_out + gate_out_lora_B * self.alpha\n",
    "        \n",
    "        up_out = torch._grouped_mm(\n",
    "            grouped_inputs,\n",
    "            self.up_proj,\n",
    "            cu_experts_count,\n",
    "        )\n",
    "        if self.up_lora is not None:\n",
    "            up_out_lora_A = torch._grouped_mm(\n",
    "                grouped_inputs,\n",
    "                self.up_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            up_out_lora_B = torch._grouped_mm(\n",
    "                up_out_lora_A,\n",
    "                self.up_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            up_out = up_out + up_out_lora_B * self.alpha\n",
    "\n",
    "        intermediate = self.act_fn(gate_out) * up_out\n",
    "        \n",
    "        down_out = torch._grouped_mm(\n",
    "            intermediate,\n",
    "            self.down_proj,\n",
    "            cu_experts_count,\n",
    "        )\n",
    "        if self.down_lora is not None:\n",
    "            down_out_lora_A = torch._grouped_mm(\n",
    "                intermediate,\n",
    "                self.down_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            down_out_lora_B = torch._grouped_mm(\n",
    "                down_out_lora_A,\n",
    "                self.down_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            down_out = down_out + down_out_lora_B * self.alpha\n",
    "\n",
    "        down_out = down_out * topk_weights.view(-1)[sort_indices].unsqueeze(-1)\n",
    "\n",
    "        outputs = hidden_states.new_zeros(M, hidden_dim)\n",
    "        sorted_pos_expanded = sorted_pos.unsqueeze(-1).expand(-1, hidden_dim)\n",
    "        outputs.scatter_add_(0, sorted_pos_expanded, down_out.to(outputs.dtype))\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n",
    "        \n",
    "        router_logits = self.gate(hidden_states_flat)\n",
    "\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        \n",
    "        if self.norm_topk_prob:\n",
    "            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "\n",
    "        final_hidden_states = self.moe(hidden_states_flat, selected_experts, routing_weights)\n",
    "        \n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        return final_hidden_states, router_logits\n",
    "\n",
    "modeling_qwen3_moe.Qwen3MoeSparseMoeBlock = Qwen3MoeSparseMoeBlockParallel\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_features = linear.in_features\n",
    "        out_features = linear.out_features\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(r, in_features, dtype=torch.bfloat16))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r, dtype=torch.bfloat16))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            # lora_B stays zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        lora_out = F.linear(F.linear(x.to(self.lora_A.dtype), self.lora_A), self.lora_B) * self.scaling\n",
    "        return out + lora_out.to(out.dtype)\n",
    "\n",
    "def check_fn(module):\n",
    "    return isinstance(module, (Qwen3MoeDecoderLayer, Qwen3MoeMLP, Qwen3MoeSparseMoeBlockParallel))\n",
    "\n",
    "non_reentrant_wrapper = partial(\n",
    "    checkpoint_wrapper,\n",
    "    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8278621-129d-464d-9d91-f1193f0cc6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 1] Running on rank 1 on device cuda:1\n",
      "[GPU 2] Running on rank 2 on device cuda:2\n",
      "[GPU 4] Running on rank 4 on device cuda:4\n",
      "[GPU 7] Running on rank 7 on device cuda:7\n",
      "[GPU 6] Running on rank 6 on device cuda:6\n",
      "[GPU 0] Running on rank 0 on device cuda:0\n",
      "[GPU 3] Running on rank 3 on device cuda:3\n",
      "[GPU 5] Running on rank 5 on device cuda:5\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "world_size = int(os.environ['WORLD_SIZE'])\n",
    "device_type = torch.accelerator.current_accelerator()\n",
    "device = torch.device(f\"{device_type}:{rank}\")\n",
    "torch.accelerator.device_index(rank)\n",
    "torch.cuda.set_device(rank)\n",
    "print(f\"Running on rank {rank} on device {device}\")\n",
    "\n",
    "num_threads = os.cpu_count() // (\n",
    "    torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    ")\n",
    "torch.set_num_threads(num_threads)\n",
    "device_mesh = init_device_mesh(device_type.type, (world_size,), mesh_dim_names=(\"dp\",))\n",
    "tp_mesh = device_mesh[\"dp\"]\n",
    "dp_mesh = device_mesh[\"dp\"]\n",
    "dp_rank = dp_mesh.get_local_rank()\n",
    "dp_world_size = dp_mesh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08584548-fa5a-47c8-92d6-b7949d0e347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "model_name = \"ramdisk/Qwen3-30B-A3B-Instruct-2507-stack\"\n",
    "model = Model.from_pretrained(\n",
    "    model_name, \n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "selected = [\n",
    "    \"q_proj\", \n",
    "    \"k_proj\", \n",
    "    \"v_proj\", \n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "rank_lora = 256\n",
    "alpha_lora = 512\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    for child_name, child in module.named_children():\n",
    "        if len(child_name) and any([a in child_name for a in selected]) and isinstance(child, nn.Linear):\n",
    "            \n",
    "            if 'mlp.experts' in name:\n",
    "                continue\n",
    "\n",
    "            lora = LinearLoRA(child, r=rank_lora, alpha=alpha_lora)\n",
    "            setattr(module, child_name, lora)\n",
    "\n",
    "top_k = model.config.num_experts_per_tok\n",
    "r = rank_lora // top_k\n",
    "alpha = alpha_lora // top_k\n",
    "\n",
    "for module in tqdm(model.modules()):\n",
    "    if isinstance(module, Qwen3MoeSparseMoeBlockParallel):\n",
    "        module.apply_lora_stack(r=r, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b08144e-d972-4087-846b-4d318853cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "fsdp_kwargs = {}\n",
    "fsdp_kwargs[\"mp_policy\"] = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "fsdp_kwargs[\"mesh\"] = dp_mesh\n",
    "\n",
    "for module in tqdm(model.modules()):\n",
    "    if isinstance(module, Qwen3MoeDecoderLayer):\n",
    "        fully_shard(module, **fsdp_kwargs)\n",
    "fully_shard(model, **fsdp_kwargs)\n",
    "\n",
    "apply_activation_checkpointing(\n",
    "    model,\n",
    "    checkpoint_wrapper_fn=non_reentrant_wrapper,\n",
    "    check_fn=check_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77225727-ba93-4a21-9c9c-4cb978ba4632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 7] {'loss': tensor(0.4108, device='cuda:7')}\n",
      "[GPU 2] {'loss': tensor(0.4108, device='cuda:2')}\n",
      "[GPU 1] {'loss': tensor(0.4108, device='cuda:1')}\n",
      "[GPU 3] {'loss': tensor(0.4108, device='cuda:3')}\n",
      "[GPU 0] {'loss': tensor(0.4108, device='cuda:0')}\n",
      "[GPU 5] {'loss': tensor(0.4108, device='cuda:5')}\n",
      "[GPU 6] {'loss': tensor(0.4108, device='cuda:6')}\n",
      "[GPU 4] {'loss': tensor(0.4108, device='cuda:4')}\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "dataset = Dataset('multipacking-qwen3')\n",
    "\n",
    "b = [dataset[0], dataset[1]]\n",
    "b = collator(b)\n",
    "with torch.no_grad():\n",
    "    for k in b.keys():\n",
    "        if isinstance(b[k], torch.Tensor):\n",
    "            b[k] = b[k].to(model.device, non_blocking=True)\n",
    "\n",
    "    valid_tokens = (b['labels'] != -100).sum().item() * dp_world_size\n",
    "    b['num_items_in_batch'] = torch.tensor(valid_tokens)\n",
    "    out = model(**b, use_cache=False)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ffe64d-f0c3-443b-93d8-e34cf5c4696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ab4add-a17f-44f8-b6d2-a22d658f7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "dataset = Dataset('multipacking-qwen3')\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_world_size,\n",
    "    rank=dp_rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=5,\n",
    "    prefetch_factor=5,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "iter_train_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a08bcbc-0c7e-4d2d-be1d-7d39754b7735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 7] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 3] done backward\n",
      "[GPU 1] done backward\n",
      "[GPU 7] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 6] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 4] done grad\n",
      "[GPU 4] 0 tensor(3.2526, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.4066, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 7] done grad\n",
      "[GPU 7] 0 tensor(3.2518, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.4065, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 3] done grad\n",
      "[GPU 0] done grad\n",
      "[GPU 0] 0 tensor(3.2515, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.4064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 0 tensor(3.2519, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.4065, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 1] done grad\n",
      "[GPU 1] 0 tensor(3.2515, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.4064, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 2] done grad\n",
      "[GPU 2] 0 tensor(3.2517, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.4065, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 5] done grad\n",
      "[GPU 5] 0 tensor(3.2512, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.4064, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 6] done grad\n",
      "[GPU 6] 0 tensor(3.2525, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.4066, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 1] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 1] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 3] done backward\n",
      "[GPU 7] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 6] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 1] done grad\n",
      "[GPU 3] done grad\n",
      "[GPU 0] done grad\n",
      "[GPU 7] done grad\n",
      "[GPU 2] done grad\n",
      "[GPU 1] 1 tensor(2.3595, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.2949, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 1 tensor(2.3594, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.2949, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 1 tensor(2.3592, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.2949, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 1 tensor(2.3602, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.2950, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 1 tensor(2.3597, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.2950, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 6] done grad\n",
      "[GPU 5] done grad\n",
      "[GPU 6] 1 tensor(2.3596, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.2949, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 4] done grad\n",
      "[GPU 5] 1 tensor(2.3593, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.2949, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 1 tensor(2.3597, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.2950, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 0] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 7] done backward\n",
      "[GPU 3] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 1] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 6] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 7] done grad\n",
      "[GPU 3] done grad\n",
      "[GPU 0] done grad\n",
      "[GPU 1] done grad\n",
      "[GPU 2] done grad\n",
      "[GPU 1] 2 tensor(1.7797, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 5] done grad\n",
      "[GPU 6] done grad\n",
      "[GPU 0] 2 tensor(1.7801, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 2 tensor(1.7798, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 2 tensor(1.7799, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 2 tensor(1.7797, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 6] 2 tensor(1.7804, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 5] 2 tensor(1.7797, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 4] done grad\n",
      "[GPU 4] 2 tensor(1.7796, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.2225, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 0] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 1] done backward\n",
      "[GPU 3] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 7] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 6] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 1] done grad\n",
      "[GPU 3] done grad\n",
      "[GPU 0] done grad\n",
      "[GPU 7] done grad\n",
      "[GPU 2] done grad\n",
      "[GPU 3] 3 tensor(1.2791, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 0] 3 tensor(1.2792, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 3 tensor(1.2788, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.1598, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 3 tensor(1.2792, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 5] done grad\n",
      "[GPU 1] 3 tensor(1.2790, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 6] done grad\n",
      "[GPU 4] done grad\n",
      "[GPU 5] 3 tensor(1.2792, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 6] 3 tensor(1.2794, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 3 tensor(1.2791, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.1599, device='cuda:4', grad_fn=<DivBackward0>)\n",
      "[GPU 3] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 7] done backward\n",
      "[GPU 3] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 1] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 6] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 7] done grad\n",
      "[GPU 3] done grad\n",
      "[GPU 0] done grad\n",
      "[GPU 1] done grad\n",
      "[GPU 2] done grad\n",
      "[GPU 5] done grad\n",
      "[GPU 0] 4 tensor(1.1125, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[GPU 1] 4 tensor(1.1126, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:1', grad_fn=<DivBackward0>)\n",
      "[GPU 2] 4 tensor(1.1127, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "[GPU 3] 4 tensor(1.1126, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:3', grad_fn=<DivBackward0>)\n",
      "[GPU 7] 4 tensor(1.1127, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "[GPU 6] done grad\n",
      "[GPU 5] 4 tensor(1.1126, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:5', grad_fn=<DivBackward0>)\n",
      "[GPU 4] done grad\n",
      "[GPU 6] 4 tensor(1.1125, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:6', grad_fn=<DivBackward0>)\n",
      "[GPU 4] 4 tensor(1.1124, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.1391, device='cuda:4', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "    \n",
    "for i in range(5):\n",
    "    total_tokens = 0\n",
    "    b = next(iter_train_loader)\n",
    "    for k in b.keys():\n",
    "        if isinstance(b[k], torch.Tensor):\n",
    "            b[k] = b[k].to(device, non_blocking=True)\n",
    "\n",
    "    valid_tokens = (b['labels'] != -100).sum().item()\n",
    "    total_tokens += valid_tokens\n",
    "    token_tensor = torch.tensor([total_tokens], dtype=torch.long, device=device)\n",
    "    dp_group = dp_mesh.get_group()\n",
    "    dist.all_reduce(token_tensor, op=dist.ReduceOp.SUM, group=dp_group)\n",
    "    global_total_tokens = token_tensor.item()\n",
    "    b['num_items_in_batch'] = torch.tensor(global_total_tokens)\n",
    "    out = model(**b, use_cache=False)\n",
    "    loss = out[\"loss\"] * dp_world_size\n",
    "    print('done forward')\n",
    "    loss.backward()\n",
    "    print('done backward')\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    print('done grad')\n",
    "\n",
    "    print(i, loss, out[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
