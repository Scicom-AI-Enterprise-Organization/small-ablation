{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41237e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/home/husein/ssd4/scicom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5ec3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.shared_memory import SharedMemory\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b31efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fully implemented multiprocess First-Fit-Decreasing (FFD) bin packer.\n",
    "\n",
    "Design choices:\n",
    "- Read Parquet with pyarrow (zero-copy where possible).\n",
    "- Store numeric lengths in a shared-memory NumPy array so workers don't pickle 18M Python objects.\n",
    "- Split work by index ranges (no dicts are sent to workers).\n",
    "- Each worker performs a LOCAL FFD on its range and returns:\n",
    "    - full_bins: list of bins (lists of indices) whose used == MAX_SIZE (these are kept as-is)\n",
    "    - partial_items: flat list of item indices that belong to partially-filled bins\n",
    "- Main process collects all full_bins and all partial_items, then runs a GLOBAL FFD *only* on partial_items.\n",
    "- Final output: combined list of bins (each bin is a list of item indices). Map back to original data as needed.\n",
    "\n",
    "This implementation is careful about memory, avoids heavy pickling, and keeps the heavy numeric array in shared memory.\n",
    "\n",
    "Requirements: pyarrow, numpy\n",
    "\n",
    "Run example:\n",
    "    python multiprocess_ffd.py /path/to/file.parquet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.shared_memory import SharedMemory\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_SIZE = 10240\n",
    "\n",
    "def local_ffd_range(args):\n",
    "    start, end, shm_name, n_items, dtype_str, lengths_min = args\n",
    "\n",
    "    shm = SharedMemory(name=shm_name)\n",
    "    arr = np.ndarray((n_items,), dtype=np.dtype(dtype_str), buffer=shm.buf)\n",
    "\n",
    "    indices = np.arange(start, end, dtype=np.int64)\n",
    "    order = np.argsort(-arr[indices])\n",
    "    ordered_idx = indices[order]\n",
    "\n",
    "    # FFD\n",
    "    bins = []\n",
    "    used = []\n",
    "\n",
    "    for idx in tqdm(ordered_idx):\n",
    "        L = int(arr[idx])\n",
    "        placed = False\n",
    "        for b, cap in enumerate(used):\n",
    "            if cap + L <= MAX_SIZE:\n",
    "                bins[b].append(int(idx))\n",
    "                used[b] += L\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            bins.append([int(idx)])\n",
    "            used.append(L)\n",
    "\n",
    "    full_bins = []\n",
    "    partial_items = []\n",
    "\n",
    "    for b, cap in zip(bins, used):\n",
    "        if (cap + lengths_min) >= MAX_SIZE:\n",
    "            full_bins.append(b)\n",
    "        else:\n",
    "            partial_items.extend(b)\n",
    "\n",
    "    return full_bins, partial_items\n",
    "\n",
    "\n",
    "def pack_ffd_indices(indices: List[int], lengths_array: np.ndarray, max_size: int = MAX_SIZE) -> List[List[int]]:\n",
    "    \"\"\"Global FFD taking item indices and using lengths_array for sizes.\n",
    "\n",
    "    Returns bins as list of lists of indices.\n",
    "    \"\"\"\n",
    "    if not indices:\n",
    "        return []\n",
    "\n",
    "    # Sort indices by descending length\n",
    "    ordered = sorted(indices, key=lambda i: int(lengths_array[i]), reverse=True)\n",
    "\n",
    "    bins: List[List[int]] = []\n",
    "    used: List[int] = []\n",
    "\n",
    "    for idx in tqdm(ordered):\n",
    "        l = int(lengths_array[idx])\n",
    "        placed = False\n",
    "        for b, cap in enumerate(used):\n",
    "            if cap + l <= max_size:\n",
    "                bins[b].append(idx)\n",
    "                used[b] += l\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            bins.append([idx])\n",
    "            used.append(l)\n",
    "\n",
    "    return bins\n",
    "\n",
    "\n",
    "def parallel_ffd(lengths: np.ndarray, cores: int = None, max_size: int = MAX_SIZE) -> List[List[int]]:\n",
    "    \"\"\"Top-level parallel FFD.\n",
    "\n",
    "    lengths: NumPy 1D array of integers (np.int32 or compatible)\n",
    "    cores: number of worker processes\n",
    "\n",
    "    Returns final bins as lists of item indices.\n",
    "    \"\"\"\n",
    "    n = lengths.shape[0]\n",
    "    if cores is None:\n",
    "        cores = max(1, os.cpu_count() - 1)\n",
    "\n",
    "    lengths_min = lengths.min()\n",
    "    dtype = lengths.dtype\n",
    "    shm = SharedMemory(create=True, size=lengths.nbytes)\n",
    "    try:\n",
    "        arr = np.ndarray(lengths.shape, dtype=dtype, buffer=shm.buf)\n",
    "        arr[:] = lengths[:]\n",
    "        \n",
    "        print('done sharedmemory')\n",
    "\n",
    "        per = math.ceil(n / cores)\n",
    "        ranges = []\n",
    "        for i in range(0, n, per):\n",
    "            ranges.append((i, min(n, i + per)))\n",
    "            \n",
    "        jobs = [\n",
    "            (start, end, shm.name, n, str(dtype), lengths_min)\n",
    "            for start, end in ranges\n",
    "        ]\n",
    "\n",
    "        with Pool(cores) as p:\n",
    "            results = p.map(local_ffd_range, jobs)\n",
    "            \n",
    "        full_bins_all: List[List[int]] = []\n",
    "        partial_items_all: List[int] = []\n",
    "\n",
    "        for full_bins, partial_items in results:\n",
    "            full_bins_all.extend(full_bins)\n",
    "            partial_items_all.extend(partial_items)\n",
    "\n",
    "        partial_bins = pack_ffd_indices(partial_items_all, arr, max_size=max_size)\n",
    "\n",
    "        final_bins = full_bins_all + partial_bins\n",
    "        return final_bins\n",
    "\n",
    "    finally:\n",
    "        # Cleanup shared memory\n",
    "        shm.close()\n",
    "        shm.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15ea38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "f = hf_hub_download(\n",
    "    repo_id=\"Scicom-intl/sort-multilingual-tts\", \n",
    "    filename=\"sort-merge.parquet\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ab916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>l</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-0</td>\n",
       "      <td>324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-0</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-0</td>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-0</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                f    l  i\n",
       "0  tokenized-4k-qwen3/tokenized-0  130  0\n",
       "1  tokenized-4k-qwen3/tokenized-0  324  1\n",
       "2  tokenized-4k-qwen3/tokenized-0   77  2\n",
       "3  tokenized-4k-qwen3/tokenized-0  174  3\n",
       "4  tokenized-4k-qwen3/tokenized-0  110  4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(f)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51a480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = df['l'].to_numpy(dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2503100b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = parallel_ffd(lengths, cores = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4ec693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614552"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488b27d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[669100, 673184, 669097, 669098, 673186, 669099, 673185, 9680]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34410e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>l</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>669100</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1719</td>\n",
       "      <td>205023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673184</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1613</td>\n",
       "      <td>209107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669097</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1511</td>\n",
       "      <td>205020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669098</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1415</td>\n",
       "      <td>205021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673186</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1293</td>\n",
       "      <td>209109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669099</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1288</td>\n",
       "      <td>205022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673185</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-1</td>\n",
       "      <td>1240</td>\n",
       "      <td>209108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>tokenized-4k-qwen3/tokenized-0</td>\n",
       "      <td>161</td>\n",
       "      <td>9680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     f     l       i\n",
       "669100  tokenized-4k-qwen3/tokenized-1  1719  205023\n",
       "673184  tokenized-4k-qwen3/tokenized-1  1613  209107\n",
       "669097  tokenized-4k-qwen3/tokenized-1  1511  205020\n",
       "669098  tokenized-4k-qwen3/tokenized-1  1415  205021\n",
       "673186  tokenized-4k-qwen3/tokenized-1  1293  209109\n",
       "669099  tokenized-4k-qwen3/tokenized-1  1288  205022\n",
       "673185  tokenized-4k-qwen3/tokenized-1  1240  209108\n",
       "9680    tokenized-4k-qwen3/tokenized-0   161    9680"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[bins[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0ee63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bin-packing.json', 'w') as fopen:\n",
    "    json.dump(bins, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54464d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hf upload Scicom-intl/sort-multilingual-tts bin-packing.json --repo-type=dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
