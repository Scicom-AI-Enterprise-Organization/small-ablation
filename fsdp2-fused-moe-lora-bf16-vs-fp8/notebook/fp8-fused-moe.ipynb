{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ab9384-8f16-4e3e-b596-d0f85a7452f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f82617-7cdf-4f74-90ae-26ceeee517c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchao.prototype.moe_training.scaled_grouped_mm import _to_fp8_rowwise_then_scaled_grouped_mm\n",
    "from torchao.prototype.moe_training import scaled_grouped_mm\n",
    "\n",
    "def pad_for_alignment(grouped_inputs, experts_count, alignment=16):\n",
    "    \"\"\"Pad inputs so each expert group is aligned. Returns padding context for reuse.\"\"\"\n",
    "    experts_count_padded = ((experts_count + alignment - 1) // alignment) * alignment\n",
    "    \n",
    "    # cu_experts_count = experts_count.cumsum(dim=0).to(torch.int32)\n",
    "    cu_original = torch.cat([torch.zeros(1, dtype=torch.int32, device=experts_count.device), \n",
    "                             experts_count.cumsum(0).to(torch.int32)])\n",
    "    cu_padded = torch.cat([torch.zeros(1, dtype=torch.int32, device=experts_count.device), \n",
    "                           experts_count_padded.cumsum(0).to(torch.int32)])\n",
    "    \n",
    "    total_tokens = grouped_inputs.shape[0]\n",
    "    token_indices = torch.arange(total_tokens, device=grouped_inputs.device)\n",
    "    expert_ids = torch.searchsorted(cu_original[1:], token_indices, right=True)\n",
    "    position_in_group = token_indices - cu_original[expert_ids]\n",
    "    dest_indices = cu_padded[expert_ids] + position_in_group\n",
    "    \n",
    "    total_padded = cu_padded[-1]\n",
    "    padded_inputs = torch.zeros(total_padded, grouped_inputs.shape[1], \n",
    "                                dtype=grouped_inputs.dtype, device=grouped_inputs.device)\n",
    "    padded_inputs[dest_indices] = grouped_inputs\n",
    "    \n",
    "    ctx = {\n",
    "        'cu_original': cu_original[1:],  # For unpadded LoRA ops\n",
    "        'cu_padded': cu_padded[1:],\n",
    "        'dest_indices': dest_indices,\n",
    "        'total_padded': total_padded,\n",
    "        'total_tokens': total_tokens,\n",
    "    }\n",
    "    return padded_inputs, ctx\n",
    "\n",
    "\n",
    "def pad_tensor(tensor, ctx):\n",
    "    padded = torch.zeros(ctx['total_padded'], tensor.shape[1], \n",
    "                         dtype=tensor.dtype, device=tensor.device)\n",
    "    padded[ctx['dest_indices']] = tensor\n",
    "    return padded\n",
    "\n",
    "\n",
    "def unpad_tensor(padded_tensor, ctx):\n",
    "    return padded_tensor[ctx['dest_indices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e1833d-8976-442a-8123-a73ba41f3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 4\n",
    "num_experts = 32\n",
    "norm_topk_prob = True\n",
    "alignment = 16\n",
    "\n",
    "hidden_states = torch.randn(32, 100, 1024, dtype=torch.bfloat16).cuda()\n",
    "batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "inputs = hidden_states.view(-1, hidden_dim)\n",
    "M = inputs.shape[0]\n",
    "w = torch.randn(32, 512, 1024, dtype=torch.bfloat16).cuda().transpose(-1, -2)\n",
    "gate = nn.Linear(1024, 32, bias=False, device='cuda', dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58052388-cb1d-4ff4-a6f3-58cfb3dbdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_proj_stacked_t = nn.Parameter(w.clone())\n",
    "\n",
    "router_logits = gate(inputs)\n",
    "routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "topk_weights, topk_indices = torch.topk(routing_weights, top_k, dim=-1)\n",
    "if norm_topk_prob:\n",
    "    topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "topk_weights = topk_weights.to(inputs.dtype)\n",
    "sort_indices = topk_indices.view(-1).argsort()\n",
    "sorted_pos = sort_indices // top_k\n",
    "grouped_inputs = inputs[sorted_pos]\n",
    "\n",
    "experts_count = topk_indices.view(-1).bincount(minlength=num_experts)\n",
    "\n",
    "padded_inputs, pad_ctx = pad_for_alignment(grouped_inputs, experts_count, alignment=16)\n",
    "cu_experts_padded = pad_ctx['cu_padded']\n",
    "cu_experts_original = pad_ctx['cu_original']\n",
    "\n",
    "o = _to_fp8_rowwise_then_scaled_grouped_mm(\n",
    "    padded_inputs,\n",
    "    gate_proj_stacked_t,\n",
    "    cu_experts_padded,\n",
    ")\n",
    "\n",
    "# Unpad outputs (no CPU-GPU sync)\n",
    "o_unpadded = unpad_tensor(o, pad_ctx)\n",
    "o_unpadded.backward(torch.ones_like(o_unpadded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25fcf4cf-89d3-4d8f-9249-24e709dfcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_proj_stacked_t_ = nn.Parameter(w.clone())\n",
    "\n",
    "router_logits = gate(inputs.clone())\n",
    "routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "topk_weights, topk_indices = torch.topk(routing_weights, top_k, dim=-1)\n",
    "if norm_topk_prob:\n",
    "    topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "topk_weights = topk_weights.to(inputs.dtype)\n",
    "sort_indices = topk_indices.view(-1).argsort()  # (M * topk,)\n",
    "sorted_pos = sort_indices // top_k\n",
    "grouped_inputs = inputs[sorted_pos]  # (M * topk, dim)\n",
    "\n",
    "experts_count = topk_indices.view(-1).bincount(minlength=num_experts)\n",
    "cu_experts_count = experts_count.cumsum(dim=0).to(torch.int32)\n",
    "o = torch._grouped_mm(\n",
    "    grouped_inputs,\n",
    "    gate_proj_stacked_t_,\n",
    "    cu_experts_original,\n",
    ")\n",
    "o.backward(torch.ones_like(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a21ef1f-51e3-4109-9821-3d532f244232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o - o_unpadded).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f74c868-c5ba-4ad5-bb19-d18e70a1f6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gate_proj_stacked_t_.grad - gate_proj_stacked_t.grad).abs().max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
