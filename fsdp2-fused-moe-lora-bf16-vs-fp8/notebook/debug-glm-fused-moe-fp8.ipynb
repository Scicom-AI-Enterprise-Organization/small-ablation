{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778b5996-d057-47e2-821a-c6c3b7f41e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef62617b-07e8-4dac-9ea7-584a71d3cda1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 78293.67it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 61680.94it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 78085.45it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 75865.96it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 71610.07it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 68120.95it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 70071.90it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 60411.79it/s]\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 89786.32it/s]\n",
      "Fetching 7 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 9338.46it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 12939.68it/s]\n",
      "\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 62468.36it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 62735.32it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 56570.57it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 950.44it/s]\n",
      "1063it [00:02, 455.82it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 992.60it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 921.15it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 910.05it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 883.91it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 946.68it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 873.47it/s]\n",
      "1063it [00:02, 509.02it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 756.24it/s]\n",
      "1063it [00:02, 501.12it/s]\n",
      "1063it [00:02, 510.46it/s]\n",
      "1063it [00:02, 526.56it/s]\n",
      "1063it [00:02, 528.03it/s]\n",
      "1063it [00:02, 524.13it/s]\n",
      "1063it [00:01, 544.54it/s]\n",
      "1198it [00:07, 160.23it/s]\n",
      "1198it [00:07, 158.42it/s]\n",
      "1198it [00:07, 157.55it/s]\n",
      "1198it [00:07, 157.34it/s]\n",
      "1198it [00:07, 156.98it/s]\n",
      "1198it [00:07, 156.75it/s]\n",
      "1198it [00:07, 155.60it/s]\n",
      "1198it [00:07, 155.42it/s]\n",
      "1198it [00:40, 29.40it/s]\n",
      "1198it [00:41, 29.17it/s]\n",
      "1198it [00:44, 27.22it/s]\n",
      "1198it [00:44, 27.04it/s]\n",
      "1198it [00:44, 27.04it/s]\n",
      "1198it [00:45, 26.35it/s]\n",
      "1198it [00:45, 26.27it/s]\n",
      "1198it [00:45, 26.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from jupytertracerviz import init_multigpus_repl, multigpus\n",
    "\n",
    "init_multigpus_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066131ba-c756-4d26-a4f3-cfc7e3d7a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 242it [00:29, 13.00it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803c4317-bd2d-4b28-8d5e-4dd6c97f75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "import torch\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing,\n",
    "    checkpoint_wrapper,\n",
    ")\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.tensor import distribute_tensor\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import fully_shard, MixedPrecisionPolicy, CPUOffloadPolicy\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Glm4MoeForCausalLM,\n",
    ")\n",
    "from transformers.models.glm4_moe.modeling_glm4_moe import (\n",
    "    Glm4MoeMLP,\n",
    "    Glm4MoeDecoderLayer,\n",
    "    Glm4MoeTopkRouter,\n",
    "    ACT2FN,\n",
    ")\n",
    "from transformers.models.glm4_moe import modeling_glm4_moe\n",
    "from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "from streaming import LocalDataset\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torchao.prototype.moe_training.scaled_grouped_mm import _to_fp8_rowwise_then_scaled_grouped_mm\n",
    "from torchao.float8 import convert_to_float8_training, Float8LinearConfig\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, folder, sequence_length=16384):\n",
    "        self.dataset = LocalDataset(local=folder)\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        data.pop('audio', None)\n",
    "        data.pop('text', None)\n",
    "        data.pop('token_type_ids', None)\n",
    "\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].astype(np.int64)\n",
    "\n",
    "        data['labels'] = data['input_ids'].copy()\n",
    "        attention_mask_sum = data['attention_mask'].sum()\n",
    "        \n",
    "        if attention_mask_sum < self.sequence_length:\n",
    "            balance = self.sequence_length - attention_mask_sum\n",
    "            data['input_ids'] = np.concatenate([data['input_ids'], np.array([151329] * balance)])\n",
    "            data['position_ids'] = np.concatenate([data['position_ids'], np.array([0] * balance)])\n",
    "            data['labels'] = np.concatenate([data['labels'], np.array([-100] * balance)])\n",
    "            data['attention_mask'] = np.concatenate([data['attention_mask'], np.array([balance])])\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def collator(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    position_ids = [b['position_ids'] for b in batch]\n",
    "    labels = [b['labels'] for b in batch]\n",
    "    attention_mask = [b['attention_mask'] for b in batch]\n",
    "    input_ids = np.concatenate(input_ids)\n",
    "    position_ids = np.concatenate(position_ids)\n",
    "    labels = np.concatenate(labels)\n",
    "    query_lens = np.concatenate(attention_mask)\n",
    "    cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "    max_cumsum = int(np.max(cumsum))\n",
    "    cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    cu_seq_lens_k = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    max_seqlen_q = int(np.max(query_lens))\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[None],\n",
    "        'position_ids': torch.tensor(position_ids)[None],\n",
    "        'labels': torch.tensor(labels)[None],\n",
    "        'cu_seq_lens_q': cu_seq_lens_q,\n",
    "        'cu_seq_lens_k': cu_seq_lens_k,\n",
    "        'max_length_q': max_seqlen_q,\n",
    "        'max_length_k': max_seqlen_q\n",
    "    }\n",
    "\n",
    "class Model(Glm4MoeForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.loss = LigerFusedLinearCrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        position_ids=None, \n",
    "        labels=None, \n",
    "        num_items_in_batch=None, \n",
    "        logits_to_keep=0,\n",
    "        output_router_logits=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super_out = self.model.forward(\n",
    "            input_ids = input_ids,\n",
    "            position_ids = position_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            output_router_logits=output_router_logits,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if labels is not None:\n",
    "            embeddings = super_out.last_hidden_state\n",
    "            slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "            embeddings = embeddings[:, slice_indices, :]\n",
    "            embeddings = embeddings[:,:-1].reshape(-1, embeddings.shape[-1])\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            labels = labels.reshape(-1)\n",
    "            \n",
    "            loss = self.loss(self.lm_head.weight, embeddings, labels)\n",
    "            num_items_in_batch = num_items_in_batch.to(loss.device)\n",
    "\n",
    "            loss = loss / num_items_in_batch\n",
    "            return {'loss': loss}\n",
    "        return super_out\n",
    "\n",
    "class ExpertLoRAWeights(nn.Module):\n",
    "    \"\"\"Wrapper to make expert LoRA weights more FSDP-friendly\"\"\"\n",
    "    def __init__(self, num_experts, in_dim, out_dim, r, dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.zeros(num_experts, in_dim, r, dtype=dtype))\n",
    "        self.B = nn.Parameter(torch.zeros(num_experts, r, out_dim, dtype=dtype))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "            \n",
    "def _to_column_major(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.transpose(-1, -2).contiguous()\n",
    "\n",
    "def pad_for_alignment(grouped_inputs, experts_count, alignment=16):\n",
    "    \"\"\"Pad inputs so each expert group is aligned. Returns padding context for reuse.\"\"\"\n",
    "    experts_count_padded = ((experts_count + alignment - 1) // alignment) * alignment\n",
    "    \n",
    "    # cu_experts_count = experts_count.cumsum(dim=0).to(torch.int32)\n",
    "    cu_original = torch.cat([torch.zeros(1, dtype=torch.int32, device=experts_count.device), \n",
    "                             experts_count.cumsum(0).to(torch.int32)])\n",
    "    cu_padded = torch.cat([torch.zeros(1, dtype=torch.int32, device=experts_count.device), \n",
    "                           experts_count_padded.cumsum(0).to(torch.int32)])\n",
    "    \n",
    "    total_tokens = grouped_inputs.shape[0]\n",
    "    token_indices = torch.arange(total_tokens, device=grouped_inputs.device)\n",
    "    expert_ids = torch.searchsorted(cu_original[1:], token_indices, right=True)\n",
    "    position_in_group = token_indices - cu_original[expert_ids]\n",
    "    dest_indices = cu_padded[expert_ids] + position_in_group\n",
    "    \n",
    "    total_padded = cu_padded[-1]\n",
    "    padded_inputs = torch.zeros(total_padded, grouped_inputs.shape[1], \n",
    "                                dtype=grouped_inputs.dtype, device=grouped_inputs.device)\n",
    "    padded_inputs[dest_indices] = grouped_inputs\n",
    "    \n",
    "    ctx = {\n",
    "        'cu_original': cu_original[1:],\n",
    "        'cu_padded': cu_padded[1:],\n",
    "        'dest_indices': dest_indices,\n",
    "        'total_padded': total_padded,\n",
    "        'total_tokens': total_tokens,\n",
    "    }\n",
    "    return padded_inputs, ctx\n",
    "\n",
    "def pad_tensor(tensor, ctx):\n",
    "    padded = torch.zeros(ctx['total_padded'], tensor.shape[1], \n",
    "                         dtype=tensor.dtype, device=tensor.device)\n",
    "    padded[ctx['dest_indices']] = tensor\n",
    "    return padded\n",
    "\n",
    "def unpad_tensor(padded_tensor, ctx):\n",
    "    return padded_tensor[ctx['dest_indices']]\n",
    "\n",
    "class Glm4MoeMoEExpertParallel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.n_routed_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.norm_topk_prob = config.norm_topk_prob\n",
    "\n",
    "        self.gate_proj = nn.Parameter(torch.zeros(self.num_experts, config.moe_intermediate_size, config.hidden_size))\n",
    "        self.up_proj = nn.Parameter(torch.zeros(self.num_experts, config.moe_intermediate_size, config.hidden_size))\n",
    "        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, config.hidden_size, config.moe_intermediate_size))\n",
    "        self._is_stacked = False\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "        self.gate = Glm4MoeTopkRouter(config)\n",
    "        self.shared_experts = Glm4MoeMLP(\n",
    "            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n",
    "        )\n",
    "        self.gate_lora = None\n",
    "        self.up_lora = None\n",
    "        self.down_lora = None\n",
    "    \n",
    "    def apply_lora_stack(self, r, alpha):\n",
    "        if self._is_stacked:\n",
    "            return\n",
    "\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.alpha = alpha / r\n",
    "        \n",
    "        self._is_stacked = True\n",
    "\n",
    "        self.gate_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.gate_proj.shape[2], self.gate_proj.shape[1], r\n",
    "        )\n",
    "        self.up_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.up_proj.shape[2], self.up_proj.shape[1], r\n",
    "        )\n",
    "        self.down_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.down_proj.shape[2], self.down_proj.shape[1], r\n",
    "        )\n",
    "        \n",
    "    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n",
    "        inputs = hidden_states\n",
    "        M = hidden_states.shape[0]\n",
    "        hidden_dim = hidden_states.shape[-1]\n",
    "\n",
    "        sort_indices = topk_indices.view(-1).argsort()  # (M * topk,)\n",
    "        sorted_pos = sort_indices // self.top_k\n",
    "        grouped_inputs = inputs[sorted_pos]  # (M * topk, dim)\n",
    "\n",
    "        experts_count = topk_indices.view(-1).bincount(minlength=self.num_experts)\n",
    "        cu_experts_count = experts_count.cumsum(dim=0).to(torch.int32)\n",
    "\n",
    "        padded_inputs, pad_ctx = pad_for_alignment(grouped_inputs, experts_count, alignment=16)\n",
    "        cu_experts_padded = pad_ctx['cu_padded']\n",
    "\n",
    "        gate_out = _to_fp8_rowwise_then_scaled_grouped_mm(\n",
    "            padded_inputs,\n",
    "            self.gate_proj.transpose(-1, -2),\n",
    "            cu_experts_padded,\n",
    "        )\n",
    "        if self.gate_lora is not None:\n",
    "            gate_out_lora_A = torch._grouped_mm(\n",
    "                grouped_inputs,\n",
    "                self.gate_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            gate_out_lora_B = torch._grouped_mm(\n",
    "                gate_out_lora_A,\n",
    "                self.gate_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            gate_out = gate_out + pad_tensor(gate_out_lora_B, pad_ctx) * self.alpha\n",
    "        \n",
    "        up_out = _to_fp8_rowwise_then_scaled_grouped_mm(\n",
    "            padded_inputs,\n",
    "            self.up_proj.transpose(-1, -2),\n",
    "            cu_experts_padded,\n",
    "        )\n",
    "        if self.up_lora is not None:\n",
    "            up_out_lora_A = torch._grouped_mm(\n",
    "                grouped_inputs,\n",
    "                self.up_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            up_out_lora_B = torch._grouped_mm(\n",
    "                up_out_lora_A,\n",
    "                self.up_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            up_out = up_out + pad_tensor(up_out_lora_B, pad_ctx) * self.alpha\n",
    "        \n",
    "        intermediate_padded = self.act_fn(gate_out) * up_out\n",
    "        \n",
    "        down_out_padded = _to_fp8_rowwise_then_scaled_grouped_mm(\n",
    "            intermediate_padded,\n",
    "            self.down_proj.transpose(-1, -2),\n",
    "            cu_experts_padded,\n",
    "        )\n",
    "\n",
    "        if self.down_lora is not None:\n",
    "            intermediate_unpadded = unpad_tensor(intermediate_padded, pad_ctx)\n",
    "            down_out_lora_A = torch._grouped_mm(\n",
    "                intermediate_unpadded,\n",
    "                self.down_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            down_out_lora_B = torch._grouped_mm(\n",
    "                down_out_lora_A,\n",
    "                self.down_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            down_out_padded = down_out_padded + pad_tensor(down_out_lora_B, pad_ctx) * self.alpha\n",
    "\n",
    "        down_out = unpad_tensor(down_out_padded, pad_ctx)\n",
    "        down_out = down_out * topk_weights.view(-1)[sort_indices].unsqueeze(-1)\n",
    "\n",
    "        outputs = inputs.new_zeros(M, hidden_dim)\n",
    "        sorted_pos_expanded = sorted_pos.unsqueeze(-1).expand(-1, hidden_dim)\n",
    "        outputs.scatter_add_(0, sorted_pos_expanded, down_out.to(outputs.dtype))\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        residuals = hidden_states\n",
    "        orig_shape = hidden_states.shape\n",
    "        topk_indices, topk_weights = self.gate(hidden_states)\n",
    "        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n",
    "        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n",
    "        hidden_states = hidden_states + self.shared_experts(residuals)\n",
    "        return hidden_states\n",
    "\n",
    "modeling_glm4_moe.Glm4MoeMoE = Glm4MoeMoEExpertParallel\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_features = linear.in_features\n",
    "        out_features = linear.out_features\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(r, in_features, dtype=torch.bfloat16))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r, dtype=torch.bfloat16))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            # lora_B stays zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        lora_out = F.linear(F.linear(x.to(self.lora_A.dtype), self.lora_A), self.lora_B) * self.scaling\n",
    "        return out + lora_out.to(out.dtype)\n",
    "\n",
    "def check_fn(module):\n",
    "    return isinstance(module, (Glm4MoeDecoderLayer, Glm4MoeMoEExpertParallel))\n",
    "    \n",
    "non_reentrant_wrapper = partial(\n",
    "    checkpoint_wrapper,\n",
    "    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df98006-c952-4c0a-a547-84b1bc5fcb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 6] Running on rank 6 on device cuda:6\n",
      "[GPU 3] Running on rank 3 on device cuda:3\n",
      "[GPU 2] Running on rank 2 on device cuda:2\n",
      "[GPU 0] Running on rank 0 on device cuda:0\n",
      "[GPU 4] Running on rank 4 on device cuda:4\n",
      "[GPU 1] Running on rank 1 on device cuda:1\n",
      "[GPU 7] Running on rank 7 on device cuda:7\n",
      "[GPU 5] Running on rank 5 on device cuda:5\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "world_size = int(os.environ['WORLD_SIZE'])\n",
    "device_type = torch.accelerator.current_accelerator()\n",
    "device = torch.device(f\"{device_type}:{rank}\")\n",
    "torch.accelerator.device_index(rank)\n",
    "torch.cuda.set_device(rank)\n",
    "print(f\"Running on rank {rank} on device {device}\")\n",
    "\n",
    "\n",
    "num_threads = os.cpu_count() // (\n",
    "    torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    ")\n",
    "torch.set_num_threads(num_threads)\n",
    "\n",
    "device_mesh = init_device_mesh(device_type.type, (world_size,), mesh_dim_names=(\"dp\",))\n",
    "dp_mesh = device_mesh[\"dp\"]\n",
    "dp_rank = dp_mesh.get_local_rank()\n",
    "dp_world_size = dp_mesh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b458707-357e-44d9-aba7-45a6122f38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "model_name = \"GLM-4.5-Air-non-transpose\"\n",
    "model = Model.from_pretrained(\n",
    "    model_name, \n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "selected = [\n",
    "    \"q_proj\", \n",
    "    \"k_proj\", \n",
    "    \"v_proj\", \n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "rank_lora = 256\n",
    "alpha_lora = 512\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    for child_name, child in module.named_children():\n",
    "        if len(child_name) and any([a in child_name for a in selected]) and isinstance(child, nn.Linear):\n",
    "            \n",
    "            if 'mlp.experts' in name:\n",
    "                continue\n",
    "\n",
    "            lora = LinearLoRA(child, r=rank_lora, alpha=alpha_lora)\n",
    "            setattr(module, child_name, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc942eb-b8df-4afd-9766-211346cc11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "top_k = model.config.num_experts_per_tok\n",
    "r = rank_lora // top_k\n",
    "alpha = alpha_lora // top_k\n",
    "\n",
    "for module in tqdm(model.modules()):\n",
    "    if isinstance(module, Glm4MoeMoEExpertParallel):\n",
    "        module.apply_lora_stack(r=r, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff64894-a8aa-4c6d-bf36-65219b5d6ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "def module_filter_fn(mod: torch.nn.Module, fqn: str):\n",
    "    if fqn == \"1\":\n",
    "        return False\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "        if mod.in_features % 16 != 0 or mod.out_features % 16 != 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "config = Float8LinearConfig.from_recipe_name(\"rowwise\")\n",
    "torch._inductor.config.emulate_precision_casts = True\n",
    "convert_to_float8_training(model, config=config, module_filter_fn=module_filter_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad57668c-cc76-464d-b942-acef7948e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 1] True True\n",
      "[GPU 5] True True\n",
      "[GPU 0] True True\n",
      "[GPU 3] True True\n",
      "[GPU 7] True True\n",
      "[GPU 4] True True\n",
      "[GPU 2] True True\n",
      "[GPU 6] True True\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "from torchao.prototype.moe_training.utils import (\n",
    "    _is_column_major,\n",
    ")\n",
    "\n",
    "print(_is_column_major(model.model.layers[1].mlp.gate_proj.transpose(1, 2)), model.model.layers[1].mlp.gate_proj.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba1e5c8d-9e72-457e-ac2b-9ec1a3ccad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "fsdp_kwargs = {}\n",
    "fsdp_kwargs[\"mp_policy\"] = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "fsdp_kwargs[\"mesh\"] = dp_mesh\n",
    "\n",
    "for module in tqdm(model.modules()):\n",
    "    if isinstance(module, Glm4MoeDecoderLayer):\n",
    "        fully_shard(module, **fsdp_kwargs)\n",
    "fully_shard(model, **fsdp_kwargs)\n",
    "\n",
    "apply_activation_checkpointing(\n",
    "    model,\n",
    "    checkpoint_wrapper_fn=non_reentrant_wrapper,\n",
    "    check_fn=check_fn,\n",
    ")\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2add70d-2176-44b6-9db7-c519ddb2c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%multigpus\n",
    "\n",
    "# dataset = Dataset('multipacking-glm')\n",
    "\n",
    "# b = [dataset[0], dataset[1]]\n",
    "# b = collator(b)\n",
    "# with torch.no_grad():\n",
    "#     for k in b.keys():\n",
    "#         if isinstance(b[k], torch.Tensor):\n",
    "#             b[k] = b[k].to(model.device, non_blocking=True)\n",
    "\n",
    "#     valid_tokens = (b['labels'] != -100).sum().item() * dp_world_size\n",
    "#     b['num_items_in_batch'] = torch.tensor(valid_tokens)\n",
    "#     out = model(**b, use_cache=False)\n",
    "\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "400bd7ea-35a7-40ed-aba2-460e4e1da5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61c7f93-8fa9-4ca3-9c3d-1d44f7c150e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "dataset = Dataset('multipacking-glm')\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_world_size,\n",
    "    rank=dp_rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=5,\n",
    "    prefetch_factor=5,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "iter_train_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a0166-7cb3-4478-aa18-22cc08f99ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 7] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 7] done backward\n",
      "[GPU 1] done backward\n",
      "[GPU 3] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 6] done backward\n",
      "[GPU 4] done grad\n",
      "[GPU 2] done grad\n",
      "[GPU 1] done grad\n",
      "[GPU 5] done grad\n",
      "[GPU 7] done grad\n",
      "[GPU 6] done grad\n",
      "[GPU 4] 0 tensor(4.1049, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.5131, device='cuda:4', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 1] 0 tensor(4.1063, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.5133, device='cuda:1', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 2] 0 tensor(4.1061, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.5133, device='cuda:2', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 5] 0 tensor(4.1048, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.5131, device='cuda:5', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 3] done grad\n",
      "[GPU 7] 0 tensor(4.1065, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.5133, device='cuda:7', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 6] 0 tensor(4.1059, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.5132, device='cuda:6', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 3] 0 tensor(4.1067, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.5133, device='cuda:3', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 0] done grad\n",
      "[GPU 0] 0 tensor(4.1063, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.5133, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 5] done forward\n",
      "[GPU 3] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 7] done forward\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_grad_norm_(parameters, max_norm, norm_type=2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    else:\n",
    "        parameters = list(parameters)\n",
    "    \n",
    "    grads = [p.grad for p in parameters if p.grad is not None]\n",
    "    \n",
    "    norms = []\n",
    "    for grad in grads:\n",
    "        if hasattr(grad, 'full_tensor'):  # DTensor\n",
    "            grad_full = grad.full_tensor()\n",
    "        else:\n",
    "            grad_full = grad\n",
    "        \n",
    "        if norm_type == float('inf'):\n",
    "            norms.append(grad_full.abs().max())\n",
    "        else:\n",
    "            norms.append(grad_full.norm(norm_type))\n",
    "    \n",
    "    if len(norms) == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "    total_norm = torch.stack(norms).norm(norm_type)\n",
    "    \n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
    "    \n",
    "    for grad in grads:\n",
    "        if hasattr(grad, 'full_tensor'):  # DTensor\n",
    "            grad.mul_(clip_coef_clamped)\n",
    "        else:\n",
    "            grad.mul_(clip_coef_clamped)\n",
    "    \n",
    "    return total_norm\n",
    "    \n",
    "for i in range(5):\n",
    "    total_tokens = 0\n",
    "    b = next(iter_train_loader)\n",
    "    for k in b.keys():\n",
    "        if isinstance(b[k], torch.Tensor):\n",
    "            b[k] = b[k].to(device, non_blocking=True)\n",
    "\n",
    "    valid_tokens = (b['labels'] != -100).sum().item()\n",
    "    total_tokens += valid_tokens\n",
    "    token_tensor = torch.tensor([total_tokens], dtype=torch.long, device=device)\n",
    "    dp_group = dp_mesh.get_group()\n",
    "    dist.all_reduce(token_tensor, op=dist.ReduceOp.SUM, group=dp_group)\n",
    "    global_total_tokens = token_tensor.item()\n",
    "    b['num_items_in_batch'] = torch.tensor(global_total_tokens)\n",
    "    out = model(**b, use_cache=False)\n",
    "    loss = out[\"loss\"] * dp_world_size\n",
    "    print('done forward')\n",
    "    loss.backward()\n",
    "    print('done backward')\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    print('done grad')\n",
    "\n",
    "    print(i, loss, out[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
