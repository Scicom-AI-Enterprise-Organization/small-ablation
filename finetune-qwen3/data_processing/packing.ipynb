{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2cbfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/mesolitica/Malaysian-SFT/resolve/main/combine/combined-malaysian-sft-10k-sample.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083875e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-30B-A3B-Instruct-2507')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65757d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = []\n",
    "with open('combined-malaysian-sft-10k-sample.jsonl') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        combine.append(l)\n",
    "\n",
    "len(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loop(files, block_size = 7168):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-8k/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "            prompt = tokenizer.apply_chat_template(row, tokenize=False)\n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            temp.append(outputs['input_ids'])\n",
    "            position_ids.append(range(len(outputs['input_ids'])))\n",
    "            count += len(outputs['input_ids'])\n",
    "            while count >= block_size:\n",
    "                block, temp = slice_and_balance(temp, block_size)\n",
    "                block_position, position_ids = slice_and_balance(position_ids, block_size)\n",
    "                count = count - block_size\n",
    "                o = collator(block, block_position)\n",
    "                last_block = block\n",
    "                last_position_block = block_position\n",
    "                out.write(o)\n",
    "                \n",
    "        block, _ = slice_and_balance(last_block, block_size - count)\n",
    "        block_position, _ = slice_and_balance(last_position_block, block_size - count)\n",
    "\n",
    "        block.extend(temp)\n",
    "        block_position.extend(position_ids)\n",
    "\n",
    "        o = collator(block, block_position)\n",
    "        if len(o['input_ids']) == block_size:\n",
    "            out.write(o)\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "chunks = chunks(combine, 50000)\n",
    "pool = Pool(10)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c394c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted(glob('tokenized-8k/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with MDSWriter(out='packing-8k', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LocalDataset('packing-8k')\n",
    "(len(dataset) * 3072) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(dataset[-3]['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac116",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(dataset[-2]['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf packing-8k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151afb9c",
   "metadata": {},
   "source": [
    "Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Simulate the environment from latest.ipynb\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B-Base\")\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning enables computers to learn from data.\",\n",
    "    \"Natural language processing helps computers understand human language.\",\n",
    "    \"Large language models are trained on billions of tokens.\",\n",
    "]\n",
    "\n",
    "block_size = 40  # small to visualize clearly\n",
    "\n",
    "# 1️⃣ Tokenize each sample\n",
    "tokenized = [tokenizer(t, add_special_tokens=False)[\"input_ids\"] for t in texts]\n",
    "print(\"---- Original Samples ----\")\n",
    "for i, t in enumerate(tokenized):\n",
    "    print(f\"Sample {i}: len={len(t)} tokens\")\n",
    "\n",
    "# 2️⃣ Exact packing (same logic as latest.ipynb)\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first, balance, current_size = [], [], 0\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remain = size - current_size\n",
    "            if len(sublist) <= remain:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remain])\n",
    "                balance.append(sublist[remain:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    return first, balance\n",
    "\n",
    "def collator(batch):\n",
    "    input_ids, position_ids, masks = [], [], []\n",
    "    for seg in batch:\n",
    "        L = len(seg)\n",
    "        input_ids.extend(seg)\n",
    "        position_ids.extend(range(L))\n",
    "        masks.append(L)\n",
    "    return {\n",
    "        \"input_ids\": np.array(input_ids, np.uint32),\n",
    "        \"position_ids\": np.array(position_ids, np.uint32),\n",
    "        \"attention_mask\": np.array(masks, np.uint32),\n",
    "    }\n",
    "\n",
    "def pack_exact(samples, block_size=40):\n",
    "    temp = []\n",
    "    count = 0\n",
    "    records = []\n",
    "    for s in samples:\n",
    "        temp.append(s)\n",
    "        count += len(s)\n",
    "        while count >= block_size:\n",
    "            block, temp = slice_and_balance(temp, block_size)\n",
    "            rec = collator(block)\n",
    "            records.append(rec)\n",
    "            count -= block_size\n",
    "    # handle leftover\n",
    "    if temp:\n",
    "        rec = collator(temp)\n",
    "        if len(rec[\"input_ids\"]) == block_size:\n",
    "            records.append(rec)\n",
    "    return records\n",
    "\n",
    "records = pack_exact(tokenized, block_size)\n",
    "\n",
    "# 3️⃣ Inspect the result\n",
    "print(\"\\n---- Packed Output ----\")\n",
    "for i, rec in enumerate(records):\n",
    "    print(f\"Record {i}: len(input_ids)={len(rec['input_ids'])}\")\n",
    "    print(f\"  attention_mask (segment lengths) = {rec['attention_mask'].tolist()}\")\n",
    "    print(f\"  position_ids (first 20) = {rec['position_ids'][:20].tolist()}\")\n",
    "\n",
    "# 4️⃣ Show overall token count conservation\n",
    "total_input_tokens = sum(len(s) for s in tokenized)\n",
    "total_packed_tokens = sum(len(r[\"input_ids\"]) for r in records)\n",
    "print(f\"\\nOriginal total tokens = {total_input_tokens}\")\n",
    "print(f\"Total written tokens  = {total_packed_tokens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
