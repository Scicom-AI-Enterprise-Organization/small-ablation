# /bin/bash
# CUDA 12.8
# Pytorch 2.7.1
# Python >3.9
wget https://huggingface.co/datasets/mesolitica/Flash-Attention3-whl/resolve/main/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64-2.7.1-12.8.whl -O flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl
pip install flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl


