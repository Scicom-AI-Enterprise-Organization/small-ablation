{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1094f714-2841-4905-b5e0-f2fb91f02ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d0eaf4-88ca-4964-9d8b-79631a1f6b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from glob import glob\n",
    "from transformers import GptOssForCausalLM, AutoModelForCausalLM, AutoTokenizer, Mxfp4Config\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac353f2a-518b-429e-8c38-0353b6373821",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = {}\n",
    "f = os.path.join(get_last_checkpoint('malaysian-reasoning-20b-lora-r64-experts'), 'weight.pt')\n",
    "with safe_open(f, framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1f0cc3-5734-4660-b3be-09101eab1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6cf6b2-ced1-4ffa-9d95-57a6b9b3bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 87122.04it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 103017.99it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 332.37it/s]\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = dict(\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    use_cache=True, \n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/gpt-oss-20b-BF16\", **model_kwargs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d68d4ac-44f2-418f-91d6-72be6d4ad144",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db358e19-62f7-4238-9b79-4ab360918bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0156,  0.0234,  ...,  0.0625,  0.0156, -0.0078],\n",
       "        [ 0.0000, -0.0000,  0.0156,  ..., -0.0625,  0.0078, -0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0234,  ...,  0.0078,  0.0312,  0.0312],\n",
       "        ...,\n",
       "        [ 0.0312, -0.0625, -0.0000,  ...,  0.0312,  0.0625,  0.0469],\n",
       "        [-0.0000, -0.0938, -0.0000,  ...,  0.0625,  0.0312,  0.0469],\n",
       "        [ 0.0625, -0.0625, -0.0156,  ..., -0.0469, -0.0312, -0.0938]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[f'model.layers.0.mlp.experts.gate_up_proj'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91390201-5c9e-43a6-8bc9-feb8ed637ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rank = 64\n",
    "top_k = model.config.num_experts_per_tok\n",
    "r = total_rank // top_k\n",
    "alpha = (total_rank * 2) // top_k\n",
    "merge_scale = alpha / r\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    W = state_dict[f'model.layers.{i}.mlp.experts.gate_up_proj']\n",
    "    A = tensors[f'model.layers.{i}.mlp.experts.lora_gate_up_A.e.weight'].to(W.device)\n",
    "    B = tensors[f'model.layers.{i}.mlp.experts.lora_gate_up_B.e.weight'].to(W.device)\n",
    "    for k in range(model.config.num_local_experts):\n",
    "        a = A[k].reshape(-1, r)\n",
    "        b = B[k].reshape(r, -1)\n",
    "\n",
    "        m = torch.matmul(a, b) * merge_scale\n",
    "        W[k] += m.to(W.dtype)\n",
    "\n",
    "    W = state_dict[f'model.layers.{i}.mlp.experts.down_proj']\n",
    "    A = tensors[f'model.layers.{i}.mlp.experts.lora_down_A.e.weight'].to(W.device)\n",
    "    B = tensors[f'model.layers.{i}.mlp.experts.lora_down_B.e.weight'].to(W.device)\n",
    "    for k in range(model.config.num_local_experts):\n",
    "        a = A[k].reshape(-1, r)\n",
    "        b = B[k].reshape(r, -1)\n",
    "\n",
    "        m = torch.matmul(a, b) * merge_scale\n",
    "        W[k] += m.to(W.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea0ef37-e212-4e7e-a1fb-e776db0b7834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0002,  0.0140,  0.0193,  ...,  0.0625,  0.0125, -0.0081],\n",
       "        [-0.0011, -0.0045,  0.0108,  ..., -0.0625,  0.0050, -0.0029],\n",
       "        [ 0.0005,  0.0016,  0.0258,  ...,  0.0080,  0.0327,  0.0322],\n",
       "        ...,\n",
       "        [ 0.0309, -0.0635, -0.0009,  ...,  0.0311,  0.0623,  0.0459],\n",
       "        [-0.0005, -0.0957, -0.0016,  ...,  0.0623,  0.0308,  0.0454],\n",
       "        [ 0.0625, -0.0620, -0.0138,  ..., -0.0466, -0.0300, -0.0938]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[f'model.layers.0.mlp.experts.gate_up_proj'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163530c4-ef68-4c44-bc19-b1c23ee850d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0064,  0.0077, -0.0791,  ...,  0.0330,  0.0342,  0.0432],\n",
       "        [-0.0146, -0.0332,  0.0131,  ..., -0.0352, -0.0640, -0.0188],\n",
       "        [-0.0142,  0.0364,  0.0317,  ..., -0.0077,  0.0544, -0.0325],\n",
       "        ...,\n",
       "        [-0.0120, -0.0071, -0.0017,  ...,  0.0173, -0.0374,  0.0400],\n",
       "        [-0.0219,  0.0141, -0.0359,  ...,  0.0256,  0.0449, -0.0396],\n",
       "        [ 0.0254, -0.0132, -0.0476,  ...,  0.0330, -0.0322, -0.0256]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['model.layers.9.self_attn.v_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb7c71d0-7ac5-4429-b1e6-034c590bb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = tensors.keys()\n",
    "keys_lora = sorted(list(set([k.split('.lora')[0] for k in keys if '.self_attn.' in k])))\n",
    "for k in keys_lora:\n",
    "    k_ori = k + '.weight'\n",
    "    post_A = '.lora_A.e.weight'\n",
    "    post_B = '.lora_B.e.weight'\n",
    "    A = k + post_A\n",
    "    B = k + post_B\n",
    "    W = state_dict[k_ori]\n",
    "    A = tensors[A].type(W.dtype).to(W.device)\n",
    "    B = tensors[B].type(W.dtype).to(W.device)\n",
    "    m = torch.matmul(A.t(), B.t()) * 2.0\n",
    "    W += m.T.to(W.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7f911b-42c3-403f-bd6e-429b06568dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0065,  0.0059, -0.0796,  ...,  0.0349,  0.0349,  0.0437],\n",
       "        [-0.0145, -0.0337,  0.0131,  ..., -0.0349, -0.0640, -0.0188],\n",
       "        [-0.0135,  0.0364,  0.0312,  ..., -0.0074,  0.0547, -0.0325],\n",
       "        ...,\n",
       "        [-0.0127, -0.0066, -0.0015,  ...,  0.0172, -0.0374,  0.0398],\n",
       "        [-0.0212,  0.0151, -0.0356,  ...,  0.0251,  0.0447, -0.0403],\n",
       "        [ 0.0253, -0.0121, -0.0471,  ...,  0.0325, -0.0322, -0.0262]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['model.layers.9.self_attn.v_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ad741b-8fa3-4635-a133-53c6d0ddae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "Pasangan algoritma yang digunakan untuk melakukan penyulitan dan nyahsulit dikenali sebagai\n",
    "A. kunci (keys)\n",
    "B. Sifer (cipher)\n",
    "C. Teks sifer (ciphertext)\n",
    "\"\"\".strip()\n",
    "\n",
    "system = 'First, you try to think step-by-step in {{lang}}, after that, put your final answer within $\\\\boxed{}$.'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system.replace('{{lang}}', 'malay')},\n",
    "    {\"role\": \"user\", \"content\": q},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb5a303-c803-4a0d-91c5-848629698aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[200006,  17360, 200008,   3575,    553,  17554, 162016,     11,    261,\n",
       "           4410,   6439,   2359,  22203,    656,   7788,  17527,    558,  87447,\n",
       "         100594,     25,    220,   1323,     19,     12,   3218,    198,   6576,\n",
       "           3521,     25,    220,   1323,     20,     12,    899,     12,   1311,\n",
       "            279,  30377,    289,     25,  14093,    279,      2,  13888,  18403,\n",
       "             25,   8450,     11,  49159,     11,   1721,     13,  21030,   2804,\n",
       "            413,   7360,    395,   1753,   3176,     13, 200007, 200006,  77944,\n",
       "         200008,      2,  68406,    279,   7127,     11,    481,   2075,    316,\n",
       "           2411,   5983,  23541,  41570,    306,   3849,    356,     11,   1934,\n",
       "            484,     11,   3006,    634,   1721,   6052,   3518, 126456, 172278,\n",
       "          12083,      3,    364, 200007, 200006,   1428, 200008,    198,  75908,\n",
       "            422,  15598,  32777,  11211,    280,     11,  22732,    516,  26322,\n",
       "         136404,  90461,     11,  85492,   6157,  10741,   7989,    272,    970,\n",
       "          15207,  11489,    364,    399,  12864,    849,   1610, 134697,    198,\n",
       "         200007, 200006, 173781, 200005,  35644, 200008]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\n",
    "\n",
    "terjemah ke kedah\n",
    "\"\"\"\n",
    "\n",
    "system = 'First, you try to think step-by-step in {{lang}}, after that, put your final answer within $\\\\boxed{}$.'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system.replace('{{lang}}', 'malay')},\n",
    "    {\"role\": \"user\", \"content\": q},\n",
    "]\n",
    "\n",
    "row = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ") + '<|channel|>analysis<|message|>'\n",
    "input_ids = tokenizer(row, add_special_tokens = False, return_tensors = 'pt').to(model.device)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9fdbc20-1ba7-4d02-a3c4-dad4430a9ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-12-13\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nFirst, you try to think step-by-step in malay, after that, put your final answer within $\\\\boxed{}$.\\n\\n<|end|><|start|>user<|message|>\\nBudak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\\n\\nterjemah ke kedah\\n<|end|><|start|>assistant<|channel|>analysis<|message|>Okay, so I have a local Malay sentence that needs to be translated into standard Kedah. The original sentence is:\\n\\n\"Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\"\\n\\nFirst, I need to understand the meaning of the entire sentence before attempting a translation. The speaker is describing someone (a child) who is very mischievous, hard to get rid of, and even if people make money (possibly by trying to control or manipulate the child), it doesn\\'t deter the child.\\n\\nBreaking it down:\\n\\n1. \"Budak itu sangat nakal\" - The child is very mischievous.\\n2. \"pantang orang leka sedikit\" - Hard to get rid of or hard to control.\\n3. \"duit syiling pun dikebasnya\" - Even if people try to control him/her with money.\\n\\nIn standard Malay, this would be something like: \"Budak itu sangat nakal, pantang digoda sedikit, duit syiling pun dikebasnya.\" But I need to translate this into standard Kedah.\\n\\nNow, considering the target language is Kedah, which has its own colloquial expressions and vocabulary. Let\\'s think about how to express each part in a way that sounds natural in Kedah.\\n\\nFirst, \"Budak itu sangat nakal\" can be translated as \"Budak tu jari jengking\" in Kedah, where \"jari jengking\" means mischievous or troublesome.\\n\\nSecond, \"pantang orang leka sedikit\" could be translated as \"pantang orang nak lehe\" in Kedah, where \"lehe\" means to get rid of or to control.\\n\\nThird, \"duit syiling pun dikebasnya\" can be translated as \"duit syiling tak nak dikebas dia\" in Kedah, meaning even if people try to control him/her with money.\\n\\nPutting it all together, the full sentence in standard Kedah would be:\\n\\n\"Budak tu jari jengking, pantang orang nak lehe, duit syiling tak nak dikebas dia.\"\\n\\nBut I should verify if \"jari jengking\" is commonly used in Kedah to mean mischievous. Upon checking, \"jari-jari\" means fingers, but \"jari jengking\" might not be standard. A more common expression in Kedah for mischievous is \"jari dikelana\" or \"jak polu\", but \"jari jengking\" might still be understandable.\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_kwargs = {\"max_new_tokens\": 512, \"do_sample\": True, \"temperature\": 0.6, \"top_p\": None, \"top_k\": None}\n",
    "\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
