{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa442d8-13eb-4c7d-86a9-e6163c104e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from glob import glob\n",
    "from transformers import GptOssForCausalLM, AutoModelForCausalLM, AutoTokenizer, Mxfp4Config\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "        \n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980bde2a-e331-48de-9304-7251455a6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(folders):\n",
    "    folders, index = folders\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(index)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        total_rank = int(folder.split('-r')[-1].split('-')[0])\n",
    "        tensors = {}\n",
    "        f = os.path.join(get_last_checkpoint(folder), 'weight.pt')\n",
    "        with safe_open(f, framework=\"pt\", device='cpu') as f:\n",
    "            for k in f.keys():\n",
    "                tensors[k] = f.get_tensor(k)\n",
    "    \n",
    "        model_kwargs = dict(\n",
    "            attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "            torch_dtype=torch.bfloat16, \n",
    "            use_cache=True, \n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"unsloth/gpt-oss-20b-BF16\", **model_kwargs).cuda()\n",
    "    \n",
    "        state_dict = model.state_dict()\n",
    "    \n",
    "        top_k = model.config.num_experts_per_tok\n",
    "        r = total_rank // top_k\n",
    "        alpha = (total_rank * 2) // top_k\n",
    "        merge_scale = alpha / r\n",
    "        \n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "            if f'model.layers.{i}.mlp.experts.lora_gate_up_A.e.weight' in tensors:\n",
    "                W = state_dict[f'model.layers.{i}.mlp.experts.gate_up_proj']\n",
    "                A = tensors[f'model.layers.{i}.mlp.experts.lora_gate_up_A.e.weight'].to(W.device)\n",
    "                B = tensors[f'model.layers.{i}.mlp.experts.lora_gate_up_B.e.weight'].to(W.device)\n",
    "                for k in range(model.config.num_local_experts):\n",
    "                    a = A[k].reshape(-1, r)\n",
    "                    b = B[k].reshape(r, -1)\n",
    "            \n",
    "                    m = torch.matmul(a, b) * merge_scale\n",
    "                    W[k] += m.to(W.dtype)\n",
    "        \n",
    "            if f'model.layers.{i}.mlp.experts.lora_down_B.e.weight' in tensors:\n",
    "                W = state_dict[f'model.layers.{i}.mlp.experts.down_proj']\n",
    "                A = tensors[f'model.layers.{i}.mlp.experts.lora_down_A.e.weight'].to(W.device)\n",
    "                B = tensors[f'model.layers.{i}.mlp.experts.lora_down_B.e.weight'].to(W.device)\n",
    "                for k in range(model.config.num_local_experts):\n",
    "                    a = A[k].reshape(-1, r)\n",
    "                    b = B[k].reshape(r, -1)\n",
    "            \n",
    "                    m = torch.matmul(a, b) * merge_scale\n",
    "                    W[k] += m.to(W.dtype)\n",
    "    \n",
    "        keys = tensors.keys()\n",
    "        keys_lora = sorted(list(set([k.split('.lora')[0] for k in keys if '.self_attn.' in k])))\n",
    "        for k in keys_lora:\n",
    "            k_ori = k + '.weight'\n",
    "            post_A = '.lora_A.e.weight'\n",
    "            post_B = '.lora_B.e.weight'\n",
    "            A = k + post_A\n",
    "            B = k + post_B\n",
    "            W = state_dict[k_ori]\n",
    "            A = tensors[A].type(W.dtype).to(W.device)\n",
    "            B = tensors[B].type(W.dtype).to(W.device)\n",
    "            m = torch.matmul(A.t(), B.t()) * 2.0\n",
    "            W += m.T.to(W.dtype)\n",
    "    \n",
    "        model.save_pretrained(f'{os.path.split(folder)[1]}-merged')\n",
    "        tokenizer.save_pretrained(f'{os.path.split(folder)[1]}-merged')\n",
    "    \n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ba88e5-c32c-4306-baff-28cf418bae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob('/root/malaysian-reasoning-20b-lora-r*experts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78743440-b38a-46e0-b41d-4ba79b85aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r128-experts\n",
      "/root/malaysian-reasoning-20b-lora-r128-selected-experts/root/malaysian-reasoning-20b-lora-r256-experts\n",
      "\n",
      "/root/malaysian-reasoning-20b-lora-r256-selected-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 77878.32it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 82704.59it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 79137.81it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 80438.71it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 109145.46it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 447.97it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 104113.93it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 20560.31it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 444.49it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 445.01it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 104857.60it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 447.91it/s]\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r32-selected-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 94710.09it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 114241.74it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 455.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r64-selected-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 93503.59it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 98523.92it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 466.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r64-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 94405.56it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 98194.41it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 387.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r16-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 101944.89it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 109145.46it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 463.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r512-selected-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 100548.38it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 121826.26it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 474.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r16-selected-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 83647.09it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 106377.28it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 455.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r32-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 93206.76it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 96898.11it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 438.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malaysian-reasoning-20b-lora-r512-experts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 97541.95it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 112923.57it/s]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 465.56it/s]\n"
     ]
    }
   ],
   "source": [
    "multiprocessing(folders, loop, cores=4, returned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64c25c1-bc72-4d8e-a5a4-38e3a7c64d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
