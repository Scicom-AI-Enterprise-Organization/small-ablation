{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb39204e-e780-4510-a876-bcf6d6712869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f630c1-2d86-4cd9-ac2c-b5cc101ffe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from glob import glob\n",
    "from transformers import GptOssForCausalLM, AutoModelForCausalLM, AutoTokenizer, Mxfp4Config\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9c48b4-8f01-451e-bfc6-02b219405579",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = {}\n",
    "f = os.path.join(get_last_checkpoint('malaysian-reasoning-120b-lora-r256-experts'), 'weight.pt')\n",
    "with safe_open(f, framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314d9aa4-3bcc-4c7b-9b76-f7491c9a1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19961711-1ac9-44d3-8606-432f30582b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 76062.51it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 112923.57it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:37<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = dict(\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    use_cache=True,\n",
    "    device_map='auto',\n",
    "    dtype='auto',\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ramdisk/gpt-oss-120b-BF16\", **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76982312-8772-419c-a63c-fd12d8831a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce05a8ea-76bd-4511-a4a4-16d66b3901d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rank = 256\n",
    "top_k = model.config.num_experts_per_tok\n",
    "r = total_rank // top_k\n",
    "alpha = (total_rank * 2) // top_k\n",
    "merge_scale = alpha / r\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    W = state_dict[f'model.layers.{i}.mlp.experts.gate_up_proj']\n",
    "    A = tensors[f'model.layers.{i}.mlp.experts.lora_gate_up_A.e.weight'].to(W.device)\n",
    "    B = tensors[f'model.layers.{i}.mlp.experts.lora_gate_up_B.e.weight'].to(W.device)\n",
    "    for k in range(model.config.num_local_experts):\n",
    "        a = A[k].reshape(-1, r)\n",
    "        b = B[k].reshape(r, -1)\n",
    "\n",
    "        m = torch.matmul(a, b) * merge_scale\n",
    "        W[k] += m.to(W.dtype)\n",
    "\n",
    "    W = state_dict[f'model.layers.{i}.mlp.experts.down_proj']\n",
    "    A = tensors[f'model.layers.{i}.mlp.experts.lora_down_A.e.weight'].to(W.device)\n",
    "    B = tensors[f'model.layers.{i}.mlp.experts.lora_down_B.e.weight'].to(W.device)\n",
    "    for k in range(model.config.num_local_experts):\n",
    "        a = A[k].reshape(-1, r)\n",
    "        b = B[k].reshape(r, -1)\n",
    "\n",
    "        m = torch.matmul(a, b) * merge_scale\n",
    "        W[k] += m.to(W.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fde91e4-df54-45b1-ac40-ae3ec79da690",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = tensors.keys()\n",
    "keys_lora = sorted(list(set([k.split('.lora')[0] for k in keys if '.self_attn.' in k])))\n",
    "for k in keys_lora:\n",
    "    k_ori = k + '.weight'\n",
    "    post_A = '.lora_A.e.weight'\n",
    "    post_B = '.lora_B.e.weight'\n",
    "    A = k + post_A\n",
    "    B = k + post_B\n",
    "    W = state_dict[k_ori]\n",
    "    A = tensors[A].type(W.dtype).to(W.device)\n",
    "    B = tensors[B].type(W.dtype).to(W.device)\n",
    "    m = torch.matmul(A.t(), B.t()) * 2.0\n",
    "    W += m.T.to(W.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "179dd4a1-9ea4-4b8e-b645-b0ef34834e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[200006,  17360, 200008,   3575,    553,  17554, 162016,     11,    261,\n",
       "           4410,   6439,   2359,  22203,    656,   7788,  17527,    558,  87447,\n",
       "         100594,     25,    220,   1323,     19,     12,   3218,    198,   6576,\n",
       "           3521,     25,    220,   1323,     20,     12,    899,     12,   1130,\n",
       "            279,  30377,    289,     25,  14093,    279,      2,  13888,  18403,\n",
       "             25,   8450,     11,  49159,     11,   1721,     13,  21030,   2804,\n",
       "            413,   7360,    395,   1753,   3176,     13, 200007, 200006,  77944,\n",
       "         200008,      2,  68406,    279,   7127,     11,    481,   2075,    316,\n",
       "           2411,   5983,  23541,  41570,    306,   3849,    356,     11,   1934,\n",
       "            484,     11,   3006,    634,   1721,   6052,   3518, 126456, 172278,\n",
       "          12083,      3,    364, 200007, 200006,   1428, 200008,    198,  75908,\n",
       "            422,  15598,  32777,  11211,    280,     11,  22732,    516,  26322,\n",
       "         136404,  90461,     11,  85492,   6157,  10741,   7989,    272,    970,\n",
       "          15207,  11489,    364,    399,  12864,    849,   1610, 134697,    198,\n",
       "         200007, 200006, 173781, 200005,  35644, 200008]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\n",
    "\n",
    "terjemah ke kedah\n",
    "\"\"\"\n",
    "\n",
    "system = 'First, you try to think step-by-step in {{lang}}, after that, put your final answer within $\\\\boxed{}$.'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system.replace('{{lang}}', 'malay')},\n",
    "    {\"role\": \"user\", \"content\": q},\n",
    "]\n",
    "\n",
    "row = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ") + '<|channel|>analysis<|message|>'\n",
    "input_ids = tokenizer(row, add_special_tokens = False, return_tensors = 'pt').to(model.device)['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b36ecc-ff66-4e63-bf4f-19786540a3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-12-30\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nFirst, you try to think step-by-step in malay, after that, put your final answer within $\\\\boxed{}$.\\n\\n<|end|><|start|>user<|message|>\\nBudak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\\n\\nterjemah ke kedah\\n<|end|><|start|>assistant<|channel|>analysis<|message|>Baik, saya akan jelaskan dengan sangat terperinci dan langkah demi langkah bagaimana ayat Bahasa Melayu standard **\"Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.\"** diterjemahkan ke dalam dialek Kedah. Saya akan menerangkan setiap perkataan, perubahan bunyi, struktur ayat, makna dan konteks budaya supaya anda dapat faham proses transformasi ayat ini dengan sangat mendalam.\\n\\n## Ayat Standard Yang Diberikan:\\n\\n**Budak itu sangat nakal, pantang orang leka sedikit, duit syiling pun dikebasnya.**\\n\\nMaksud ayat ini adalah: Anak itu sangat nakal, tidak boleh orang lengah sesaat, wang syiling pun akan dimakan/digigitnya.\\n\\n## Langkah 1: Faham Konteks dan Makna Ayat Standard\\n\\nSebelum melakukan sebarang penukaran, penting untuk memahami keseluruhan maksud dan tujuan ayat asal. Ayat ini menggambarkan sifat buruk seorang kanak-kanak yang sangat nakal dan tidak dapat dikawal, di mana bahkan perkara kecil seperti wang syiling akan dimanfaatkan tanpa izin.\\n\\n### Ringkasan Maksud:\\n\\n* **Sifat anak**: Sangat nakal.\\n* **Tindakan orang**: Tidak boleh orang lengah sedikit.\\n* **Akibat**: Duit syiling akan dimakan/digigit oleh anak tersebut.\\n\\n## Langkah 2: Terjemahan \"Budak itu\" menjadi \"Budak tu\"\\n\\n* **Budak itu**:\\n\\n  * Dalam dialek Kedah, \"itu\" sering diganti dengan \"tu\" untuk merujuk kepada sesuatu yang telah disebutkan atau diketahui.\\n  * \"Tu\" adalah bentuk ringkas dan lazim dalam pertuturan harian.\\n* **Budak tu**:\\n\\n  * \"Budak\" kekal sama.\\n  * \"Tu\" menggantikan \"itu\" untuk merujuk kepada kanak-kanak yang dikenali.\\n\\n## Langkah 3: Terjemahkan \"sangat nakal\" menjadi \"nakal betoi\"\\n\\n* **Sangat nakal**:\\n\\n  * \"Sangat\" ialah kata ganti yang menunjukkan tahap yang tinggi.\\n  * \"Nakal\" bermaksud nakal atau buruk tingkah laku.\\n* **Nakal betoi**:\\n\\n  * Dalam dialek Kedah, \"betoi\" digunakan sebagai kata penegasan yang setara dengan \"sangat\" dalam BM standard'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_kwargs = {\"max_new_tokens\": 512, \"do_sample\": True, \"temperature\": 0.6, \"top_p\": None, \"top_k\": None}\n",
    "\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21452405-ae3f-4cc4-9e32-d9ae90b947ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
