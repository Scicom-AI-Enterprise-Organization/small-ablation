{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7f9828-e8c6-488f-b8de-df926486a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92c5f91-893c-4d64-a2ff-cbb19c640542",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c36c05a1-b660-4b48-a7b3-7515a0b5a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance\n",
    "\n",
    "def loop(rows, block_size = 1024*16, folder = 'tokenized-16k'):\n",
    "    rows, index = rows\n",
    "    out_root = f'{folder}/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "            row = tokenizer.apply_chat_template(row, tokenize=False)\n",
    "            outputs = tokenizer(row, add_special_tokens = False)\n",
    "            temp.append(outputs['input_ids'])\n",
    "            position_ids.append(range(len(outputs['input_ids'])))\n",
    "            count += len(outputs['input_ids'])\n",
    "            while count >= block_size:\n",
    "                block, temp = slice_and_balance(temp, block_size)\n",
    "                block_position, position_ids = slice_and_balance(position_ids, block_size)\n",
    "                count = count - block_size\n",
    "                o = collator(block, block_position)\n",
    "                last_block = block\n",
    "                last_position_block = block_position\n",
    "                out.write(o)\n",
    "\n",
    "        try:\n",
    "            block, _ = slice_and_balance(last_block, block_size - count)\n",
    "            block_position, _ = slice_and_balance(last_position_block, block_size - count)\n",
    "    \n",
    "            block.extend(temp)\n",
    "            block_position.extend(position_ids)\n",
    "    \n",
    "            o = collator(block, block_position)\n",
    "            if len(o['input_ids']) == block_size:\n",
    "                out.write(o)\n",
    "                return o\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14b8303a-a153-4255-aa7a-19a4c8e0dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('prepared-reasoning-data.json') as fopen:\n",
    "    texts = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b9b65a0-b6b9-46b6-bd52-5f2807b1484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33717/33717 [01:53<00:00, 297.49it/s]\n"
     ]
    }
   ],
   "source": [
    "r = loop((texts, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47796620-c288-4b87-ab12-4ca29118a49e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiprocessing(texts, loop, cores = 10, returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c9baa38-92cd-4fba-a0e0-33dc8aad6883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4577"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('tokenized-16k/tokenized-0')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fba33ac-0334-4e7f-abff-76b90b4fb3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33717/33717 [01:57<00:00, 286.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([    25,    478,   4402, ...,    346,     13, 200002], dtype=uint32),\n",
       " 'position_ids': array([1328, 1329, 1330, ..., 1909, 1910, 1911], dtype=uint32),\n",
       " 'attention_mask': array([ 171,  293, 1584], dtype=uint32)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop((texts, 0), 2048, 'tokenized-2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27021961-f7fb-4b59-8efa-23a582fb1ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36610"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('tokenized-2k/tokenized-0')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6959691-54ad-40b4-9290-1c04b2854208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
