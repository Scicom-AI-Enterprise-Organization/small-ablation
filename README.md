# Small Ablation Studies

A collection of ablation studies on distributed training techniques, Mixture-of-Experts (MoE) architectures, and memory-efficient training methods for large language models.

## Overview

This repository contains practical ablation experiments conducted to validate and compare various distributed training approaches. Each study includes reproducible code, benchmark results, and analysis to help practitioners make informed decisions about training infrastructure.

## Acknowledgments

Built with insights from:
- [PyTorch Distributed](https://pytorch.org/docs/stable/distributed.html)
- [torchtitan](https://github.com/pytorch/torchtitan)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [Liger Kernel](https://github.com/linkedin/Liger-Kernel)