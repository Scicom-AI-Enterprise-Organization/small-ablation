{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf81f97-3f0c-4cea-803f-4dae8fe68ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 75475.91it/s]\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 56899.47it/s]\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 67339.74it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 67494.55it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 61294.63it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 93802.33it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 21588.33it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 56570.57it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 70577.23it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 75475.91it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 34541.33it/s]\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 5303.49it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 92.36it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 82.62it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 96.97it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 98.71it/s]\n",
      "26it [00:00, 51.07it/s]ds: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 101.65it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 96.45it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 102.99it/s]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 121.09it/s]\n",
      "1063it [00:22, 47.10it/s]\n",
      "1063it [00:24, 43.11it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1015it [00:24, 51.26it/s]\n",
      "1063it [00:24, 42.70it/s]\n",
      "1027it [00:24, 64.07it/s]\n",
      "1063it [00:24, 43.10it/s]\n",
      "1063it [00:24, 43.09it/s]\n",
      "1198it [00:14, 80.62it/s]\n",
      "1198it [00:12, 95.13it/s] \n",
      "1198it [00:12, 93.63it/s] \n",
      "1198it [00:12, 97.82it/s] \n",
      "1198it [00:12, 97.18it/s] \n",
      "1198it [00:12, 99.82it/s] \n",
      "1198it [00:11, 100.21it/s]\n",
      "1198it [00:11, 100.25it/s]\n",
      "1198it [00:43, 27.85it/s]\n",
      "1198it [00:43, 27.82it/s]\n",
      "1198it [00:43, 27.73it/s]\n",
      "\n",
      "1198it [00:45, 26.25it/s]\n",
      "1198it [00:46, 25.73it/s]\n",
      "1198it [00:47, 25.46it/s]\n",
      "1198it [00:47, 25.39it/s]\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/root/.venv/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/root/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from jupytertracerviz import init_multigpus_repl, multigpus\n",
    "\n",
    "init_multigpus_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8010e8-10a1-41b9-940d-ef3fa3f4bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "import torch\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing,\n",
    "    checkpoint_wrapper,\n",
    ")\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.tensor import distribute_tensor\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import fully_shard, MixedPrecisionPolicy, CPUOffloadPolicy\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Glm4MoeForCausalLM,\n",
    ")\n",
    "from transformers.models.glm4_moe.modeling_glm4_moe import (\n",
    "    Glm4MoeMLP,\n",
    "    Glm4MoeDecoderLayer,\n",
    "    Glm4MoeTopkRouter,\n",
    "    ACT2FN,\n",
    ")\n",
    "from transformers.models.glm4_moe import modeling_glm4_moe\n",
    "from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "from streaming import LocalDataset\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, folder, sequence_length=16384):\n",
    "        self.dataset = LocalDataset(local=folder)\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        data.pop('audio', None)\n",
    "        data.pop('text', None)\n",
    "        data.pop('token_type_ids', None)\n",
    "\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].astype(np.int64)\n",
    "\n",
    "        data['labels'] = data['input_ids'].copy()\n",
    "        attention_mask_sum = data['attention_mask'].sum()\n",
    "        \n",
    "        if attention_mask_sum < self.sequence_length:\n",
    "            balance = self.sequence_length - attention_mask_sum\n",
    "            data['input_ids'] = np.concatenate([data['input_ids'], np.array([151329] * balance)])\n",
    "            data['position_ids'] = np.concatenate([data['position_ids'], np.array([0] * balance)])\n",
    "            data['labels'] = np.concatenate([data['labels'], np.array([-100] * balance)])\n",
    "            data['attention_mask'] = np.concatenate([data['attention_mask'], np.array([balance])])\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def collator(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    position_ids = [b['position_ids'] for b in batch]\n",
    "    labels = [b['labels'] for b in batch]\n",
    "    attention_mask = [b['attention_mask'] for b in batch]\n",
    "    input_ids = np.concatenate(input_ids)\n",
    "    position_ids = np.concatenate(position_ids)\n",
    "    labels = np.concatenate(labels)\n",
    "    query_lens = np.concatenate(attention_mask)\n",
    "    cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "    max_cumsum = int(np.max(cumsum))\n",
    "    cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    cu_seq_lens_k = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    max_seqlen_q = int(np.max(query_lens))\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[None],\n",
    "        'position_ids': torch.tensor(position_ids)[None],\n",
    "        'labels': torch.tensor(labels)[None],\n",
    "        'cu_seq_lens_q': cu_seq_lens_q,\n",
    "        'cu_seq_lens_k': cu_seq_lens_k,\n",
    "        'max_length_q': max_seqlen_q,\n",
    "        'max_length_k': max_seqlen_q\n",
    "    }\n",
    "\n",
    "class Model(Glm4MoeForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.loss = LigerFusedLinearCrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        position_ids=None, \n",
    "        labels=None, \n",
    "        num_items_in_batch=None, \n",
    "        logits_to_keep=0,\n",
    "        output_router_logits=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super_out = self.model.forward(\n",
    "            input_ids = input_ids,\n",
    "            position_ids = position_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            output_router_logits=output_router_logits,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if labels is not None:\n",
    "            embeddings = super_out.last_hidden_state\n",
    "            slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "            embeddings = embeddings[:, slice_indices, :]\n",
    "            embeddings = embeddings[:,:-1].reshape(-1, embeddings.shape[-1])\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            labels = labels.reshape(-1)\n",
    "            \n",
    "            loss = self.loss(self.lm_head.weight, embeddings, labels)\n",
    "            num_items_in_batch = num_items_in_batch.to(loss.device)\n",
    "\n",
    "            loss = loss / num_items_in_batch\n",
    "            return {'loss': loss}\n",
    "        return super_out\n",
    "\n",
    "class ExpertLoRAWeights(nn.Module):\n",
    "    \"\"\"Wrapper to make expert LoRA weights more FSDP-friendly\"\"\"\n",
    "    def __init__(self, num_experts, in_dim, out_dim, r, dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.zeros(num_experts, in_dim, r, dtype=dtype))\n",
    "        self.B = nn.Parameter(torch.zeros(num_experts, r, out_dim, dtype=dtype))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "            \n",
    "class Glm4MoeMoEExpertParallel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.n_routed_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.norm_topk_prob = config.norm_topk_prob\n",
    "\n",
    "        self.gate_proj = nn.Parameter(torch.zeros(self.num_experts, config.hidden_size, config.moe_intermediate_size))\n",
    "        self.up_proj = nn.Parameter(torch.zeros(self.num_experts, config.hidden_size, config.moe_intermediate_size))\n",
    "        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, config.moe_intermediate_size, config.hidden_size))\n",
    "        self._is_stacked = False\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "        self.gate = Glm4MoeTopkRouter(config)\n",
    "        self.shared_experts = Glm4MoeMLP(\n",
    "            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n",
    "        )\n",
    "        self.gate_lora = None\n",
    "        self.up_lora = None\n",
    "        self.down_lora = None\n",
    "    \n",
    "    def apply_lora_stack(self, r, alpha):\n",
    "        if self._is_stacked:\n",
    "            return\n",
    "\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.alpha = alpha / r\n",
    "        \n",
    "        self._is_stacked = True\n",
    "\n",
    "        self.gate_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.gate_proj.shape[1], self.gate_proj.shape[2], r\n",
    "        )\n",
    "        self.up_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.up_proj.shape[1], self.up_proj.shape[2], r\n",
    "        )\n",
    "        self.down_lora = ExpertLoRAWeights(\n",
    "            self.num_experts, self.down_proj.shape[1], self.down_proj.shape[2], r\n",
    "        )\n",
    "\n",
    "    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n",
    "        inputs = hidden_states\n",
    "        M = hidden_states.shape[0]\n",
    "        hidden_dim = hidden_states.shape[-1]\n",
    "\n",
    "        sort_indices = topk_indices.view(-1).argsort()  # (M * topk,)\n",
    "        sorted_pos = sort_indices // self.top_k\n",
    "        grouped_inputs = inputs[sorted_pos]  # (M * topk, dim)\n",
    "\n",
    "        experts_count = topk_indices.view(-1).bincount(minlength=self.num_experts)\n",
    "        cu_experts_count = experts_count.cumsum(dim=0).to(torch.int32)\n",
    "\n",
    "        gate_out = torch._grouped_mm(\n",
    "            grouped_inputs,\n",
    "            self.gate_proj,\n",
    "            cu_experts_count,\n",
    "        )\n",
    "        if self.gate_lora is not None:\n",
    "            gate_out_lora_A = torch._grouped_mm(\n",
    "                grouped_inputs,\n",
    "                self.gate_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            gate_out_lora_B = torch._grouped_mm(\n",
    "                gate_out_lora_A,\n",
    "                self.gate_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            gate_out = gate_out + gate_out_lora_B * self.alpha\n",
    "        \n",
    "        up_out = torch._grouped_mm(\n",
    "            grouped_inputs,\n",
    "            self.up_proj,\n",
    "            cu_experts_count,\n",
    "        )\n",
    "        if self.up_lora is not None:\n",
    "            up_out_lora_A = torch._grouped_mm(\n",
    "                grouped_inputs,\n",
    "                self.up_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            up_out_lora_B = torch._grouped_mm(\n",
    "                up_out_lora_A,\n",
    "                self.up_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            up_out = up_out + up_out_lora_B * self.alpha\n",
    "\n",
    "        intermediate = self.act_fn(gate_out) * up_out\n",
    "        \n",
    "        down_out = torch._grouped_mm(\n",
    "            intermediate,\n",
    "            self.down_proj,\n",
    "            cu_experts_count,\n",
    "        )\n",
    "\n",
    "        if self.down_lora is not None:\n",
    "            down_out_lora_A = torch._grouped_mm(\n",
    "                intermediate,\n",
    "                self.down_lora.A,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            down_out_lora_B = torch._grouped_mm(\n",
    "                down_out_lora_A,\n",
    "                self.down_lora.B,\n",
    "                cu_experts_count,\n",
    "            )\n",
    "            down_out = down_out + down_out_lora_B * self.alpha\n",
    "\n",
    "        down_out = down_out * topk_weights.view(-1)[sort_indices].unsqueeze(-1)\n",
    "\n",
    "        outputs = inputs.new_zeros(M, hidden_dim)\n",
    "        sorted_pos_expanded = sorted_pos.unsqueeze(-1).expand(-1, hidden_dim)\n",
    "        outputs.scatter_add_(0, sorted_pos_expanded, down_out.to(outputs.dtype))\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        residuals = hidden_states\n",
    "        orig_shape = hidden_states.shape\n",
    "        topk_indices, topk_weights = self.gate(hidden_states)\n",
    "        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n",
    "        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n",
    "        hidden_states = hidden_states + self.shared_experts(residuals)\n",
    "        return hidden_states\n",
    "\n",
    "modeling_glm4_moe.Glm4MoeMoE = Glm4MoeMoEExpertParallel\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_features = linear.in_features\n",
    "        out_features = linear.out_features\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(r, in_features, dtype=torch.bfloat16))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r, dtype=torch.bfloat16))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            # lora_B stays zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        lora_out = F.linear(F.linear(x.to(self.lora_A.dtype), self.lora_A), self.lora_B) * self.scaling\n",
    "        return out + lora_out.to(out.dtype)\n",
    "        \n",
    "def check_fn(module):\n",
    "    return isinstance(module, (Glm4MoeDecoderLayer, Glm4MoeMoEExpertParallel))\n",
    "\n",
    "non_reentrant_wrapper = partial(\n",
    "    checkpoint_wrapper,\n",
    "    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n",
    ")\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "world_size = int(os.environ['WORLD_SIZE'])\n",
    "device_type = torch.accelerator.current_accelerator()\n",
    "device = torch.device(f\"{device_type}:{rank}\")\n",
    "device_mesh = init_device_mesh(device_type.type, (world_size,), mesh_dim_names=(\"dp\",))\n",
    "dp_mesh = device_mesh[\"dp\"]\n",
    "dp_rank = dp_mesh.get_local_rank()\n",
    "dp_world_size = dp_mesh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d8256f-1d1e-40e9-bf97-b90052aaa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "model_name = \"nfs/nfs/GLM-4.5-Air\"\n",
    "model = Model.from_pretrained(\n",
    "    model_name, \n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "selected = [\n",
    "    \"q_proj\", \n",
    "    \"k_proj\", \n",
    "    \"v_proj\", \n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "rank_lora = 256\n",
    "alpha_lora = 512\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    for child_name, child in module.named_children():\n",
    "        if len(child_name) and any([a in child_name for a in selected]) and isinstance(child, nn.Linear):\n",
    "            \n",
    "            if 'mlp.experts' in name:\n",
    "                continue\n",
    "\n",
    "            lora = LinearLoRA(child, r=rank_lora, alpha=alpha_lora)\n",
    "            setattr(module, child_name, lora)\n",
    "\n",
    "top_k = model.config.num_experts_per_tok\n",
    "r = rank_lora // top_k\n",
    "alpha = alpha_lora // top_k\n",
    "\n",
    "for module in tqdm(model.modules()):\n",
    "    if isinstance(module, Glm4MoeMoEExpertParallel):\n",
    "        module.apply_lora_stack(r=r, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6820c69f-566f-4a33-8ff7-3b52832f0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "# lora_mp_policy = MixedPrecisionPolicy(\n",
    "#     param_dtype=torch.float32,\n",
    "#     reduce_dtype=torch.float32,\n",
    "# )\n",
    "\n",
    "# for name, module in tqdm(model.named_modules()):\n",
    "#     for child_name, child in module.named_children():\n",
    "#         if '.lora' in name:\n",
    "#             fully_shard(child, mp_policy=lora_mp_policy)\n",
    "\n",
    "fsdp_kwargs = {}\n",
    "fsdp_kwargs[\"mp_policy\"] = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "for module in tqdm(model.modules()):\n",
    "    if isinstance(module, Glm4MoeDecoderLayer):\n",
    "        fully_shard(module, **fsdp_kwargs)\n",
    "fully_shard(model, **fsdp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad9b8c6-1ed4-45a2-a0ae-de19faaa0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "apply_activation_checkpointing(\n",
    "    model,\n",
    "    checkpoint_wrapper_fn=non_reentrant_wrapper,\n",
    "    check_fn=check_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394dc842-91bb-471c-89ad-26b32a1d2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2f110c-99d9-4d79-acdb-bfea2cdd408d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%multigpus\n",
    "\n",
    "# if rank == 0:\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.size(), param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d6c3fda-3d9d-4d17-87c1-3ceff558a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%multigpus\n",
    "\n",
    "# dataset = Dataset('multipacking-glm')\n",
    "\n",
    "# b = [dataset[0], dataset[1]]\n",
    "# b = collator(b)\n",
    "# with torch.no_grad():\n",
    "#     for k in b.keys():\n",
    "#         if isinstance(b[k], torch.Tensor):\n",
    "#             b[k] = b[k].to(model.device, non_blocking=True)\n",
    "\n",
    "#     valid_tokens = (b['labels'] != -100).sum().item() * dp_world_size\n",
    "#     b['num_items_in_batch'] = torch.tensor(valid_tokens)\n",
    "#     out = model(**b, use_cache=False)\n",
    "\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8c4ae8-836e-447f-a4fa-a96e7aced0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c08e8e5-fb21-40e3-aa00-2a3777bc01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "dataset = Dataset('multipacking-glm')\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_world_size,\n",
    "    rank=dp_rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=5,\n",
    "    prefetch_factor=5,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "iter_train_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9d565-1a29-46ec-8471-f4e86e84e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 3] done forward\n",
      "[GPU 2] done forward\n",
      "[GPU 5] done forward\n",
      "[GPU 1] done forward\n",
      "[GPU 0] done forward\n",
      "[GPU 6] done forward\n",
      "[GPU 4] done forward\n",
      "[GPU 7] done forward\n",
      "[GPU 6] done backward\n",
      "[GPU 7] done backward\n",
      "[GPU 1] done backward\n",
      "[GPU 3] done backward\n",
      "[GPU 0] done backward\n",
      "[GPU 2] done backward\n",
      "[GPU 4] done backward\n",
      "[GPU 5] done backward\n",
      "[GPU 7] done grad\n",
      "[GPU 3] done grad\n",
      "[GPU 5] done grad\n",
      "[GPU 6] done grad\n",
      "[GPU 2] done grad\n",
      "[GPU 0] done grad\n",
      "[GPU 4] done grad\n",
      "[GPU 7] 0 tensor(4.0931, device='cuda:7', grad_fn=<MulBackward0>) tensor(0.5116, device='cuda:7', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 1] done grad\n",
      "[GPU 6] 0 tensor(4.0936, device='cuda:6', grad_fn=<MulBackward0>) tensor(0.5117, device='cuda:6', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 5] 0 tensor(4.0933, device='cuda:5', grad_fn=<MulBackward0>) tensor(0.5117, device='cuda:5', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 2] 0 tensor(4.0941, device='cuda:2', grad_fn=<MulBackward0>) tensor(0.5118, device='cuda:2', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 0] 0 tensor(4.0940, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.5118, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 3] 0 tensor(4.0938, device='cuda:3', grad_fn=<MulBackward0>) tensor(0.5117, device='cuda:3', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 1] 0 tensor(4.0930, device='cuda:1', grad_fn=<MulBackward0>) tensor(0.5116, device='cuda:1', grad_fn=<CompiledFunctionBackward>)\n",
      "[GPU 4] 0 tensor(4.0930, device='cuda:4', grad_fn=<MulBackward0>) tensor(0.5116, device='cuda:4', grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_grad_norm_(parameters, max_norm, norm_type=2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    else:\n",
    "        parameters = list(parameters)\n",
    "    \n",
    "    grads = [p.grad for p in parameters if p.grad is not None]\n",
    "    \n",
    "    norms = []\n",
    "    for grad in grads:\n",
    "        if hasattr(grad, 'full_tensor'):  # DTensor\n",
    "            grad_full = grad.full_tensor()\n",
    "        else:\n",
    "            grad_full = grad\n",
    "        \n",
    "        if norm_type == float('inf'):\n",
    "            norms.append(grad_full.abs().max())\n",
    "        else:\n",
    "            norms.append(grad_full.norm(norm_type))\n",
    "    \n",
    "    if len(norms) == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "    total_norm = torch.stack(norms).norm(norm_type)\n",
    "    \n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n",
    "    \n",
    "    for grad in grads:\n",
    "        if hasattr(grad, 'full_tensor'):  # DTensor\n",
    "            grad.mul_(clip_coef_clamped)\n",
    "        else:\n",
    "            grad.mul_(clip_coef_clamped)\n",
    "    \n",
    "    return total_norm\n",
    "    \n",
    "for i in range(5):\n",
    "    total_tokens = 0\n",
    "    b = next(iter_train_loader)\n",
    "    for k in b.keys():\n",
    "        if isinstance(b[k], torch.Tensor):\n",
    "            b[k] = b[k].to(device, non_blocking=True)\n",
    "\n",
    "    valid_tokens = (b['labels'] != -100).sum().item()\n",
    "    total_tokens += valid_tokens\n",
    "    token_tensor = torch.tensor([total_tokens], dtype=torch.long, device=device)\n",
    "    dp_group = dp_mesh.get_group()\n",
    "    dist.all_reduce(token_tensor, op=dist.ReduceOp.SUM, group=dp_group)\n",
    "    global_total_tokens = token_tensor.item()\n",
    "    b['num_items_in_batch'] = torch.tensor(global_total_tokens)\n",
    "    out = model(**b, use_cache=False)\n",
    "    loss = out[\"loss\"] * dp_world_size\n",
    "    print('done forward')\n",
    "    loss.backward()\n",
    "    print('done backward')\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    print('done grad')\n",
    "\n",
    "    print(i, loss, out[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d6726-1940-4c21-b7a6-76cd5f60801a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
