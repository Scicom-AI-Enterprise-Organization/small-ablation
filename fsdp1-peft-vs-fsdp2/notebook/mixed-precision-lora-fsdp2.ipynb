{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5fc0c15-5512-4e28-a8ba-feb9e524fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from multigpus_repl import init_multigpus_repl, multigpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fc7810-ad72-4b5f-864d-34e5a54bbf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 0] Worker 0/8 initialized on GPU 0 on localhost:12355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 92038.02it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 88434.12it/s]\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 82241.25it/s]\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 76858.97it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 77263.49it/s]\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "Fetching 7 files:   0%|                                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 75089.84it/s]\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 121322.84it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 104857.60it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 25486.22it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 96262.71it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 60661.42it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 96262.71it/s]\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 85349.21it/s]\n",
      "You do not have `flash_attn` installed, using `kernels-community/vllm-flash-attn3` from the `kernels` library instead!\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 109145.46it/s]\n",
      "Fetching 7 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 111212.61it/s]\n"
     ]
    }
   ],
   "source": [
    "init_multigpus_repl(print_on_rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d30a43c-be39-41c4-a3db-1d6a897ee13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing,\n",
    "    checkpoint_wrapper,\n",
    ")\n",
    "from torch.distributed._tensor import Shard, Replicate\n",
    "from torch.distributed.tensor.parallel import (\n",
    "    parallelize_module,\n",
    "    ColwiseParallel,\n",
    "    RowwiseParallel,\n",
    "    PrepareModuleInput,\n",
    "    SequenceParallel\n",
    ")\n",
    "from torch import distributed as dist\n",
    "from torch.distributed.tensor import distribute_tensor\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import fully_shard, MixedPrecisionPolicy, CPUOffloadPolicy\n",
    "from transformers.models.qwen3.modeling_qwen3 import Qwen3DecoderLayer\n",
    "from streaming import LocalDataset\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen3ForCausalLM\n",
    "import transformers.models.qwen3.modeling_qwen3 as qwen3_modeling\n",
    "from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.dataset = LocalDataset(local=folder)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        data.pop('text', None)\n",
    "        data.pop('token_type_ids', None)\n",
    "\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].astype(np.int64)\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def collator(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    position_ids = [b['position_ids'] for b in batch]\n",
    "    labels = [b['input_ids'].copy() for b in batch]\n",
    "    attention_mask = [b['attention_mask'] for b in batch]\n",
    "    input_ids = np.concatenate(input_ids)\n",
    "    position_ids = np.concatenate(position_ids)\n",
    "    labels = np.concatenate(labels)\n",
    "    query_lens = np.concatenate(attention_mask)\n",
    "    cumsum = [0] + np.cumsum(query_lens).tolist()\n",
    "    max_cumsum = int(np.max(cumsum))\n",
    "    cu_seq_lens_q = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    cu_seq_lens_k = torch.tensor(cumsum, dtype=torch.int32)\n",
    "    max_seqlen_q = int(np.max(query_lens))\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[None],\n",
    "        'position_ids': torch.tensor(position_ids)[None],\n",
    "        'labels': torch.tensor(labels)[None],\n",
    "        'cu_seq_lens_q': cu_seq_lens_q,\n",
    "        'cu_seq_lens_k': cu_seq_lens_k,\n",
    "        'max_length_q': max_seqlen_q,\n",
    "        'max_length_k': max_seqlen_q\n",
    "    }\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        in_features = linear.in_features\n",
    "        out_features = linear.out_features\n",
    "        \n",
    "        device = self.linear.weight.device\n",
    "        dtype = self.linear.weight.dtype\n",
    "\n",
    "        self.lora_A = nn.ModuleDict({})\n",
    "        self.lora_B = nn.ModuleDict({})\n",
    "        \n",
    "        self.lora_A['e'] = nn.Linear(\n",
    "            in_features, r, bias=False, \n",
    "            device = device,\n",
    "            dtype = torch.float32,\n",
    "        )\n",
    "        self.lora_B['e'] = nn.Linear(\n",
    "            r, out_features, bias=False, \n",
    "            device = device,\n",
    "            dtype = torch.float32,\n",
    "        )\n",
    "\n",
    "        for param in self.lora_A['e'].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.lora_B['e'].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py#L260\n",
    "        init.kaiming_uniform_(self.lora_A['e'].weight, a=math.sqrt(5))\n",
    "        init.zeros_(self.lora_B['e'].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        lora_update = self.lora_B['e'](self.lora_A['e'](x.to(self.lora_A['e'].weight.dtype))) * self.scaling\n",
    "        out = out + lora_update.to(x.dtype)\n",
    "        return out\n",
    "\n",
    "class Model(Qwen3ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.loss = LigerFusedLinearCrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, position_ids=None, labels=None, **kwargs):\n",
    "        super_out = self.model.forward(\n",
    "            input_ids = input_ids,\n",
    "            position_ids = position_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            output_hidden_states = True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if labels is not None:\n",
    "            embeddings = super_out.last_hidden_state\n",
    "            embeddings = embeddings[:,:-1].reshape(-1, embeddings.shape[-1])\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            labels = labels.reshape(-1)\n",
    "            print(self.lm_head.weight.shape, type(self.lm_head.weight))\n",
    "            loss = self.loss(self.lm_head.weight, embeddings, labels)\n",
    "            return {'loss': loss}\n",
    "        return super_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed5b877-e628-4e0e-8f6f-8e1e45e4957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "device_type = torch.accelerator.current_accelerator()\n",
    "device_mesh = init_device_mesh(device_type.type, (4,), mesh_dim_names=(\"dp\",))\n",
    "dp_mesh = device_mesh[\"dp\"]\n",
    "dp_rank = dp_mesh.get_local_rank()\n",
    "dp_world_size = dp_mesh.size()\n",
    "\n",
    "dataset = Dataset('multipacking')\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_world_size,\n",
    "    rank=dp_rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=5,\n",
    "    sampler=sampler,\n",
    "    num_workers=5,\n",
    "    prefetch_factor=5,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "iter_train_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45edcce-4c4b-4138-b1c8-85798fca04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "model = Model.from_pretrained(\n",
    "    model_name, \n",
    "    attn_implementation='flash_attention_3',\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "selected = [\n",
    "    \"q_proj\", \n",
    "    \"k_proj\", \n",
    "    \"v_proj\", \n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "for name, module in model.named_modules():\n",
    "    for child_name, child in module.named_children():\n",
    "        if len(child_name) and any([a in child_name for a in selected]) and isinstance(child, nn.Linear):\n",
    "            lora = LinearLoRA(child, r=128, alpha=256)\n",
    "            setattr(module, child_name, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bb4675-c762-4ed3-af71-3c3155184b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "lora_mp_policy = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.float32,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "mp_policy = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    for module in [layer.self_attn.q_proj, layer.self_attn.k_proj,\n",
    "                   layer.self_attn.v_proj, layer.self_attn.o_proj,\n",
    "                   layer.mlp.gate_proj, layer.mlp.up_proj, layer.mlp.down_proj]:\n",
    "        fully_shard(\n",
    "            module.lora_A['e'],\n",
    "            mesh=dp_mesh,\n",
    "            mp_policy=lora_mp_policy,\n",
    "        )\n",
    "        fully_shard(\n",
    "            module.lora_B['e'],\n",
    "            mesh=dp_mesh,\n",
    "            mp_policy=lora_mp_policy,\n",
    "        )\n",
    "        fully_shard(\n",
    "            module.linear,\n",
    "            mesh=dp_mesh,\n",
    "            mp_policy=mp_policy,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3dc2b7-fb38-477e-bf3e-18e890906bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "fsdp_kwargs = {}\n",
    "fsdp_kwargs[\"mp_policy\"] = MixedPrecisionPolicy(\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    ")\n",
    "fsdp_kwargs['mesh'] = dp_mesh\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Qwen3DecoderLayer):\n",
    "        fully_shard(module, **fsdp_kwargs)\n",
    "fully_shard(model, **fsdp_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9987b41-9ccd-4399-9691-cfac20044ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%multigpus\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb71b79f-9e4e-4ce5-8585-43a24ee0fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 0] torch.bfloat16 torch.float32\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "model.model.layers[0].self_attn.q_proj.linear.weight.dtype, model.model.layers[0].self_attn.q_proj.lora_A['e'].weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d37d50c-7fda-474a-ad03-f9ebbaa152c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 0] torch.Size([151936, 1024]) torch.Size([37984, 1024]) <class 'torch.distributed.tensor.DTensor'>\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "model.lm_head.weight.shape, model.lm_head.weight._local_tensor.shape, type(model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197d0c51-2785-41a2-b102-d044a10808c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 0] torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "0 tensor(3.5573, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "1 tensor(3.1449, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "2 tensor(2.9979, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "3 tensor(3.0127, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "4 tensor(3.3525, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "5 tensor(2.8937, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "6 tensor(3.0007, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "7 tensor(2.8613, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "8 tensor(2.6603, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "torch.Size([151936, 1024]) <class 'torch.nn.parameter.Parameter'>\n",
      "9 tensor(2.9501, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "%%multigpus\n",
    "\n",
    "device = torch.device(f\"{device_type}:{rank}\")\n",
    "for i in range(10):\n",
    "    b = next(iter_train_loader)\n",
    "    for k in b.keys():\n",
    "        if isinstance(b[k], torch.Tensor):\n",
    "            b[k] = b[k].to(device, non_blocking=True)\n",
    "    \n",
    "    out = model(**b, use_cache=False)\n",
    "    out['loss'].backward()\n",
    "    optim.step()\n",
    "\n",
    "    print(i, out['loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
